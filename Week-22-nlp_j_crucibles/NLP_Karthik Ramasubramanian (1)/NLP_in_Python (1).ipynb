{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP in Python\n",
    "## Analyzing millions of Yelp reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrick Harrison presented this notebook during the [PyData DC 2016 conference](http://pydata.org/dc2016/schedule/presentation/11/). To view the video of the presentation on YouTube, see [here](https://www.youtube.com/watch?v=6zm9NC9uRkk)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics to be covered\n",
    "This tutorial features an end-to-end data science & natural language processing pipeline, starting with **raw data** and running through **preparing**, **modeling**, **visualizing**, and **analyzing** the data. We'll touch on the following points:\n",
    "1. A tour of the dataset\n",
    "1. Introduction to text processing with spaCy\n",
    "1. Automatic phrase modeling\n",
    "1. Topic modeling with LDA\n",
    "1. Visualizing topic models with pyLDAvis\n",
    "1. Word vector models with word2vec\n",
    "1. Visualizing word2vec with t-SNE\n",
    "\n",
    "...and we might even learn a thing or two about Python along the way.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Yelp Dataset\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) is a dataset published by the business review service [Yelp](http://yelp.com) for academic research and educational purposes. The dataset contains crowd sourced reviews about various business like restaurant, clinics etc\n",
    "\n",
    "The data is provided in a handful of files in _.json_ format. We'll be using the following files for our demo:\n",
    "- __yelp\\_academic\\_dataset\\_business.json__ &mdash; _the records for individual businesses_\n",
    "- __yelp\\_academic\\_dataset\\_review.json__ &mdash; _the records for reviews users wrote about businesses_\n",
    "\n",
    "The files are text files (UTF-8) with one _json object_ per line, each one corresponding to an individual data record. Let's take a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\":\"Apn5Q_b6Nz61Tq4XzPdf9A\",\"name\":\"Minhas Micro Brewery\",\"neighborhood\":\"\",\"address\":\"1314 44 Avenue NE\",\"city\":\"Calgary\",\"state\":\"AB\",\"postal_code\":\"T2E 6L6\",\"latitude\":51.0918130155,\"longitude\":-114.031674872,\"stars\":4.0,\"review_count\":24,\"is_open\":1,\"attributes\":{\"BikeParking\":\"False\",\"BusinessAcceptsCreditCards\":\"True\",\"BusinessParking\":\"{'garage': False, 'street': True, 'validated': False, 'lot': False, 'valet': False}\",\"GoodForKids\":\"True\",\"HasTV\":\"True\",\"NoiseLevel\":\"average\",\"OutdoorSeating\":\"False\",\"RestaurantsAttire\":\"casual\",\"RestaurantsDelivery\":\"False\",\"RestaurantsGoodForGroups\":\"True\",\"RestaurantsPriceRange2\":\"2\",\"RestaurantsReservations\":\"True\",\"RestaurantsTakeOut\":\"True\"},\"categories\":\"Tours, Breweries, Pizza, Restaurants, Food, Hotels & Travel\",\"hours\":{\"Monday\":\"8:30-17:0\",\"Tuesday\":\"11:0-21:0\",\"Wednesday\":\"11:0-21:0\",\"Thursday\":\"11:0-21:0\",\"Friday\":\"11:0-21:0\",\"Saturday\":\"11:0-21:0\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "data_directory = os.path.join('..', 'data')\n",
    "\n",
    "businesses_filepath = os.path.join(data_directory,\n",
    "                                   'yelp_academic_dataset_business.json')\n",
    "\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    first_business_record = f.readline() \n",
    "\n",
    "print(first_business_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business records consist of _key, value_ pairs containing information about the particular business. A few attributes we'll be interested in for this demo include:\n",
    "- __business\\_id__ &mdash; _unique identifier for businesses_\n",
    "- __categories__ &mdash; _an array containing relevant category values of businesses_\n",
    "\n",
    "The _categories_ attribute is of special interest. This demo will focus on restaurants, which are indicated by the presence of the _Restaurant_ tag in the _categories_ array. In addition, the _categories_ array may contain more detailed information about restaurants, such as the type of food they serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review records are stored in a similar manner &mdash; _key, value_ pairs containing information about the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"x7mDIiDB3jEiPGPHOmDzyw\",\"user_id\":\"msQe1u7Z_XuqjGoqhB0J5g\",\"business_id\":\"iCQpiavjjPzJ5_3gPD5Ebg\",\"stars\":2,\"date\":\"2011-02-25\",\"text\":\"The pizza was okay. Not the best I've had. I prefer Biaggio's on Flamingo \\/ Fort Apache. The chef there can make a MUCH better NY style pizza. The pizzeria @ Cosmo was over priced for the quality and lack of personality in the food. Biaggio's is a much better pick if youre going for italian - family owned, home made recipes, people that actually CARE if you like their food. You dont get that at a pizzeria in a casino. I dont care what you say...\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_json_filepath = os.path.join(data_directory,\n",
    "                                    'yelp_academic_dataset_review.json')\n",
    "\n",
    "with codecs.open(review_json_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few attributes of note on the review records:\n",
    "- __business\\_id__ &mdash; _indicates which business the review is about_\n",
    "- __text__ &mdash; _the natural language text the user wrote_\n",
    "\n",
    "The _text_ attribute will be our focus today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_json_ is a handy file format for data interchange, but it's typically not the most usable for any sort of modeling work. Let's do a bit more data preparation to get our data in a more usable format. Our next code block will do the following:\n",
    "1. Read in each business record and convert it to a Python `dict`\n",
    "2. Filter out business records that aren't about restaurants (i.e., not in the \"Restaurant\" category)\n",
    "3. Create a `frozenset` of the business IDs for restaurants, which we'll use in the next step\n",
    "\n",
    "Note: \n",
    "\n",
    "tuples are immutable lists, frozensets are immutable sets.\n",
    "\n",
    "tuples are indeed an ordered collection of objects, but they can contain duplicates and unhashable objects, and have slice functionality\n",
    "\n",
    "frozensets aren't indexed, but you have the functionality of sets - O(1) element lookups, and functionality such as unions and intersections. They also can't contain duplicates, like their mutable counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57,714 restaurants in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "# open the businesses file\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    \n",
    "    # iterate through each line (json record) in the file\n",
    "    for business_json in f:\n",
    "        \n",
    "        # convert the json record to a Python dict\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        # if this business is not a restaurant, skip to the next one\n",
    "        if business[u'categories'] is not None and u'Restaurants' not in business[u'categories']:\n",
    "            continue\n",
    "            \n",
    "        # add the restaurant business id to our restaurant_ids set\n",
    "        restaurant_ids.add(business[u'business_id'])\n",
    "\n",
    "# turn restaurant_ids into a frozenset, as we don't need to change it anymore\n",
    "restaurant_ids = frozenset(restaurant_ids)\n",
    "\n",
    "# print the number of unique restaurant ids in the dataset\n",
    "print ('{:,}'.format(len(restaurant_ids)), u'restaurants in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new file that contains only the text from reviews about restaurants, with one review per line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('..', 'intermediate')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory,\n",
    "                                   'review_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 3,658,019 restaurant reviews\n",
      "              written to the new txt file.\n",
      "Wall time: 6min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if True:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with codecs.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "        # open the existing review json file\n",
    "        with codecs.open(review_json_filepath, encoding='utf_8') as review_json_file:\n",
    "\n",
    "            # loop through all reviews in the existing file and convert to dict\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json)\n",
    "\n",
    "                # if this review is not about a restaurant, skip to the next one\n",
    "                if review[u'business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "\n",
    "                # write the restaurant review as a line in the new file\n",
    "                # escape newline characters in the original review text\n",
    "                review_txt_file.write(review[u'text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "\n",
    "    print (u'''Text from {:,} restaurant reviews\n",
    "              written to the new txt file.'''.format(review_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with codecs.open(review_txt_filepath, encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print (u'Text from {:,} restaurant reviews in the txt file.'.format(review_count + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START FROM HERE\n",
    "## spaCy &mdash; Industrial-Strength NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spaCy](https://s3.amazonaws.com/skipgram-images/spaCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**spaCy**](https://spacy.io) is an industrial-strength natural language processing (_NLP_) library for Python. spaCy's goal is to take recent advancements in natural language processing out of research papers and put them in the hands of users to build production software.\n",
    "\n",
    "spaCy handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n",
    "- Tokenization\n",
    "- Text normalization, such as lowercasing, stemming/lemmatization\n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Sentence boundary detection\n",
    "- Named entity recognition and annotation\n",
    "\n",
    "In the \"batteries included\" Python tradition, spaCy contains built-in data and models which you can use out-of-the-box for processing general-purpose English language text:\n",
    "- Large English vocabulary, including stopword lists\n",
    "- Token \"probabilities\"\n",
    "- Word vectors\n",
    "\n",
    "spaCy is written in optimized Cython, which means it's _fast_. According to a few independent sources, it's the fastest syntactic parser available in any language. Key pieces of the spaCy parsing pipeline are written in pure C, enabling efficient multithreading (i.e., spaCy can release the _GIL_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "#run this command in your terminal before using the below code: python -m spacy download en\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab a sample review to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you're looking for an adventure in Las Vegas when it comes to food, this is the place to go (and this adventure isn't for the weak!) I had to say I was extremely sketchy because you get a pound or more of crawfish freshly boiled, mixed in with their seasonings and sauces, and you have to be the one to crack them open yourself to get to the crawfish meat, but that is the whole experience and beauty of the restaurant. \n",
      "\n",
      "I wish I could try their other sauces and seasoning, but I feel even the next step up from what is considered mild is way too spicy and hot for me. Guess I need to work on becoming a novice here! Anyways, this location is always busy, and expect a bit of a wait every time you come (larger parties are always going to be an hour of more). I rate this 4 stars because this is a very interesting experience when it comes to dining! Be fearless!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_txt_filepath = \"C:\\\\Knowledge Base\\\\GreyAtom\\\\nlp-in-python-master\\\\intermediate\\\\review_text_all_small.txt\"\n",
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 9, 10))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand the review text to spaCy, and be prepared to wait..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 308 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...1/20th of a second or so. Let's take a look at what we got during that time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you're looking for an adventure in Las Vegas when it comes to food, this is the place to go (and this adventure isn't for the weak!) I had to say I was extremely sketchy because you get a pound or more of crawfish freshly boiled, mixed in with their seasonings and sauces, and you have to be the one to crack them open yourself to get to the crawfish meat, but that is the whole experience and beauty of the restaurant. \n",
      "\n",
      "I wish I could try their other sauces and seasoning, but I feel even the next step up from what is considered mild is way too spicy and hot for me. Guess I need to work on becoming a novice here! Anyways, this location is always busy, and expect a bit of a wait every time you come (larger parties are always going to be an hour of more). I rate this 4 stars because this is a very interesting experience when it comes to dining! Be fearless!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (parsed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks the same! What happened under the hood?\n",
    "\n",
    "What about sentence detection and segmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "If you're looking for an adventure in Las Vegas when it comes to food, this is the place to go (and this adventure isn't for the weak!)\n",
      "\n",
      "Sentence 2:\n",
      "I had to say I was extremely sketchy because you get a pound or more of crawfish freshly boiled, mixed in with their seasonings and sauces, and you have to be the one to crack them open yourself to get to the crawfish meat, but that is the whole experience and beauty of the restaurant. \n",
      "\n",
      "\n",
      "\n",
      "Sentence 3:\n",
      "I wish I could try their other sauces and seasoning, but I feel even the next step up from what is considered mild is way too spicy and hot for me.\n",
      "\n",
      "Sentence 4:\n",
      "Guess I need to work on becoming a novice here!\n",
      "\n",
      "Sentence 5:\n",
      "Anyways, this location is always busy, and expect a bit of a wait every time you come (larger parties are always going to be an hour of more).\n",
      "\n",
      "Sentence 6:\n",
      "I rate this 4 stars because this is a very interesting experience when it comes to dining!\n",
      "\n",
      "Sentence 7:\n",
      "Be fearless!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print ('Sentence {}:'.format(num + 1))\n",
    "    print (sentence)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about named entity detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Las Vegas - GPE\n",
      "\n",
      "Entity 2: 4 - CARDINAL\n",
      "\n",
      "Entity 3: \n",
      " - GPE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GPE - Geographical/Social/Political Entities (GPE)\n",
    "\n",
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print ('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about part of speech tagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'re</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adventure</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Las</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vegas</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>when</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>comes</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>food</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>place</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>go</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>adventure</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>n't</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>hour</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>more</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>rate</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>4</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>stars</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>because</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>very</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>interesting</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>experience</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>when</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>comes</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>to</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>dining</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>!</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Be</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>fearless</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>!</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text part_of_speech\n",
       "0             If            ADP\n",
       "1            you           PRON\n",
       "2            're           VERB\n",
       "3        looking           VERB\n",
       "4            for            ADP\n",
       "5             an            DET\n",
       "6      adventure           NOUN\n",
       "7             in            ADP\n",
       "8            Las          PROPN\n",
       "9          Vegas          PROPN\n",
       "10          when            ADV\n",
       "11            it           PRON\n",
       "12         comes           VERB\n",
       "13            to            ADP\n",
       "14          food           NOUN\n",
       "15             ,          PUNCT\n",
       "16          this            DET\n",
       "17            is           VERB\n",
       "18           the            DET\n",
       "19         place           NOUN\n",
       "20            to           PART\n",
       "21            go           VERB\n",
       "22             (          PUNCT\n",
       "23           and          CCONJ\n",
       "24          this            DET\n",
       "25     adventure           NOUN\n",
       "26            is           VERB\n",
       "27           n't            ADV\n",
       "28           for            ADP\n",
       "29           the            DET\n",
       "..           ...            ...\n",
       "162           to           PART\n",
       "163           be           VERB\n",
       "164           an            DET\n",
       "165         hour           NOUN\n",
       "166           of            ADP\n",
       "167         more            ADJ\n",
       "168            )          PUNCT\n",
       "169            .          PUNCT\n",
       "170            I           PRON\n",
       "171         rate           VERB\n",
       "172         this            DET\n",
       "173            4            NUM\n",
       "174        stars           NOUN\n",
       "175      because            ADP\n",
       "176         this            DET\n",
       "177           is           VERB\n",
       "178            a            DET\n",
       "179         very            ADV\n",
       "180  interesting            ADJ\n",
       "181   experience           NOUN\n",
       "182         when            ADV\n",
       "183           it           PRON\n",
       "184        comes           VERB\n",
       "185           to            ADP\n",
       "186       dining           NOUN\n",
       "187            !          PUNCT\n",
       "188           Be           VERB\n",
       "189     fearless            ADJ\n",
       "190            !          PUNCT\n",
       "191           \\n          SPACE\n",
       "\n",
       "[192 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_pos)),columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about text normalization, like stemming/lemmatization and shape analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If</td>\n",
       "      <td>if</td>\n",
       "      <td>Xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'re</td>\n",
       "      <td>be</td>\n",
       "      <td>'xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking</td>\n",
       "      <td>look</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adventure</td>\n",
       "      <td>adventure</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Las</td>\n",
       "      <td>las</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vegas</td>\n",
       "      <td>vegas</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>comes</td>\n",
       "      <td>come</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>place</td>\n",
       "      <td>place</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>go</td>\n",
       "      <td>go</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>adventure</td>\n",
       "      <td>adventure</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>n't</td>\n",
       "      <td>not</td>\n",
       "      <td>x'x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>hour</td>\n",
       "      <td>hour</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>more</td>\n",
       "      <td>more</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>I</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>rate</td>\n",
       "      <td>rate</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>stars</td>\n",
       "      <td>star</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>because</td>\n",
       "      <td>because</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>very</td>\n",
       "      <td>very</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>interesting</td>\n",
       "      <td>interesting</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>experience</td>\n",
       "      <td>experience</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>comes</td>\n",
       "      <td>come</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>dining</td>\n",
       "      <td>dining</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Be</td>\n",
       "      <td>be</td>\n",
       "      <td>Xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>fearless</td>\n",
       "      <td>fearless</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text  token_lemma token_shape\n",
       "0             If           if          Xx\n",
       "1            you       -PRON-         xxx\n",
       "2            're           be         'xx\n",
       "3        looking         look        xxxx\n",
       "4            for          for         xxx\n",
       "5             an           an          xx\n",
       "6      adventure    adventure        xxxx\n",
       "7             in           in          xx\n",
       "8            Las          las         Xxx\n",
       "9          Vegas        vegas       Xxxxx\n",
       "10          when         when        xxxx\n",
       "11            it       -PRON-          xx\n",
       "12         comes         come        xxxx\n",
       "13            to           to          xx\n",
       "14          food         food        xxxx\n",
       "15             ,            ,           ,\n",
       "16          this         this        xxxx\n",
       "17            is           be          xx\n",
       "18           the          the         xxx\n",
       "19         place        place        xxxx\n",
       "20            to           to          xx\n",
       "21            go           go          xx\n",
       "22             (            (           (\n",
       "23           and          and         xxx\n",
       "24          this         this        xxxx\n",
       "25     adventure    adventure        xxxx\n",
       "26            is           be          xx\n",
       "27           n't          not         x'x\n",
       "28           for          for         xxx\n",
       "29           the          the         xxx\n",
       "..           ...          ...         ...\n",
       "162           to           to          xx\n",
       "163           be           be          xx\n",
       "164           an           an          xx\n",
       "165         hour         hour        xxxx\n",
       "166           of           of          xx\n",
       "167         more         more        xxxx\n",
       "168            )            )           )\n",
       "169            .            .           .\n",
       "170            I       -PRON-           X\n",
       "171         rate         rate        xxxx\n",
       "172         this         this        xxxx\n",
       "173            4            4           d\n",
       "174        stars         star        xxxx\n",
       "175      because      because        xxxx\n",
       "176         this         this        xxxx\n",
       "177           is           be          xx\n",
       "178            a            a           x\n",
       "179         very         very        xxxx\n",
       "180  interesting  interesting        xxxx\n",
       "181   experience   experience        xxxx\n",
       "182         when         when        xxxx\n",
       "183           it       -PRON-          xx\n",
       "184        comes         come        xxxx\n",
       "185           to           to          xx\n",
       "186       dining       dining        xxxx\n",
       "187            !            !           !\n",
       "188           Be           be          Xx\n",
       "189     fearless     fearless        xxxx\n",
       "190            !            !           !\n",
       "191           \\n           \\n          \\n\n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about token-level entity analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'re</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adventure</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Las</td>\n",
       "      <td>GPE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vegas</td>\n",
       "      <td>GPE</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>when</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>comes</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>food</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>place</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>go</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>this</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>adventure</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>is</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>n't</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>be</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>an</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>hour</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>of</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>more</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>)</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>I</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>rate</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>this</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>4</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>stars</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>because</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>this</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>is</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>very</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>interesting</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>experience</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>when</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>comes</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>dining</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>!</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Be</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>fearless</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>!</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>\\n</td>\n",
       "      <td>GPE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text entity_type inside_outside_begin\n",
       "0             If                                O\n",
       "1            you                                O\n",
       "2            're                                O\n",
       "3        looking                                O\n",
       "4            for                                O\n",
       "5             an                                O\n",
       "6      adventure                                O\n",
       "7             in                                O\n",
       "8            Las         GPE                    B\n",
       "9          Vegas         GPE                    I\n",
       "10          when                                O\n",
       "11            it                                O\n",
       "12         comes                                O\n",
       "13            to                                O\n",
       "14          food                                O\n",
       "15             ,                                O\n",
       "16          this                                O\n",
       "17            is                                O\n",
       "18           the                                O\n",
       "19         place                                O\n",
       "20            to                                O\n",
       "21            go                                O\n",
       "22             (                                O\n",
       "23           and                                O\n",
       "24          this                                O\n",
       "25     adventure                                O\n",
       "26            is                                O\n",
       "27           n't                                O\n",
       "28           for                                O\n",
       "29           the                                O\n",
       "..           ...         ...                  ...\n",
       "162           to                                O\n",
       "163           be                                O\n",
       "164           an                                O\n",
       "165         hour                                O\n",
       "166           of                                O\n",
       "167         more                                O\n",
       "168            )                                O\n",
       "169            .                                O\n",
       "170            I                                O\n",
       "171         rate                                O\n",
       "172         this                                O\n",
       "173            4    CARDINAL                    B\n",
       "174        stars                                O\n",
       "175      because                                O\n",
       "176         this                                O\n",
       "177           is                                O\n",
       "178            a                                O\n",
       "179         very                                O\n",
       "180  interesting                                O\n",
       "181   experience                                O\n",
       "182         when                                O\n",
       "183           it                                O\n",
       "184        comes                                O\n",
       "185           to                                O\n",
       "186       dining                                O\n",
       "187            !                                O\n",
       "188           Be                                O\n",
       "189     fearless                                O\n",
       "190            !                                O\n",
       "191           \\n         GPE                    B\n",
       "\n",
       "[192 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "             columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about a variety of other token-level attributes, such as the relative frequency of tokens, and whether or not a token matches any of these categories?\n",
    "- stopword\n",
    "- punctuation\n",
    "- whitespace\n",
    "- represents a number\n",
    "- whether or not the token is included in spaCy's default vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'re</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adventure</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Las</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vegas</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>when</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>it</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>comes</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>food</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>place</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>to</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>go</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>and</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>this</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>adventure</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>is</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>n't</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>for</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>the</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>to</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>be</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>an</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>hour</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>of</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>more</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>)</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>I</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>rate</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>this</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>4</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>stars</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>because</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>this</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>is</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>a</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>very</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>interesting</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>experience</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>when</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>it</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>comes</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>to</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>dining</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>!</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Be</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>fearless</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>!</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>\\n</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "0             If            -20.0                                          \n",
       "1            you            -20.0   Yes                                    \n",
       "2            're            -20.0                                          \n",
       "3        looking            -20.0                                          \n",
       "4            for            -20.0   Yes                                    \n",
       "5             an            -20.0   Yes                                    \n",
       "6      adventure            -20.0                                          \n",
       "7             in            -20.0   Yes                                    \n",
       "8            Las            -20.0                                          \n",
       "9          Vegas            -20.0                                          \n",
       "10          when            -20.0   Yes                                    \n",
       "11            it            -20.0   Yes                                    \n",
       "12         comes            -20.0                                          \n",
       "13            to            -20.0   Yes                                    \n",
       "14          food            -20.0                                          \n",
       "15             ,            -20.0                Yes                       \n",
       "16          this            -20.0   Yes                                    \n",
       "17            is            -20.0   Yes                                    \n",
       "18           the            -20.0   Yes                                    \n",
       "19         place            -20.0                                          \n",
       "20            to            -20.0   Yes                                    \n",
       "21            go            -20.0   Yes                                    \n",
       "22             (            -20.0                Yes                       \n",
       "23           and            -20.0   Yes                                    \n",
       "24          this            -20.0   Yes                                    \n",
       "25     adventure            -20.0                                          \n",
       "26            is            -20.0   Yes                                    \n",
       "27           n't            -20.0                                          \n",
       "28           for            -20.0   Yes                                    \n",
       "29           the            -20.0   Yes                                    \n",
       "..           ...              ...   ...          ...         ...     ...   \n",
       "162           to            -20.0   Yes                                    \n",
       "163           be            -20.0   Yes                                    \n",
       "164           an            -20.0   Yes                                    \n",
       "165         hour            -20.0                                          \n",
       "166           of            -20.0   Yes                                    \n",
       "167         more            -20.0   Yes                                    \n",
       "168            )            -20.0                Yes                       \n",
       "169            .            -20.0                Yes                       \n",
       "170            I            -20.0                                          \n",
       "171         rate            -20.0                                          \n",
       "172         this            -20.0   Yes                                    \n",
       "173            4            -20.0                                    Yes   \n",
       "174        stars            -20.0                                          \n",
       "175      because            -20.0   Yes                                    \n",
       "176         this            -20.0   Yes                                    \n",
       "177           is            -20.0   Yes                                    \n",
       "178            a            -20.0   Yes                                    \n",
       "179         very            -20.0   Yes                                    \n",
       "180  interesting            -20.0                                          \n",
       "181   experience            -20.0                                          \n",
       "182         when            -20.0   Yes                                    \n",
       "183           it            -20.0   Yes                                    \n",
       "184        comes            -20.0                                          \n",
       "185           to            -20.0   Yes                                    \n",
       "186       dining            -20.0                                          \n",
       "187            !            -20.0                Yes                       \n",
       "188           Be            -20.0                                          \n",
       "189     fearless            -20.0                                          \n",
       "190            !            -20.0                Yes                       \n",
       "191           \\n            -20.0                            Yes           \n",
       "\n",
       "    out of vocab.?  \n",
       "0              Yes  \n",
       "1              Yes  \n",
       "2              Yes  \n",
       "3              Yes  \n",
       "4              Yes  \n",
       "5              Yes  \n",
       "6              Yes  \n",
       "7              Yes  \n",
       "8              Yes  \n",
       "9              Yes  \n",
       "10             Yes  \n",
       "11             Yes  \n",
       "12             Yes  \n",
       "13             Yes  \n",
       "14             Yes  \n",
       "15             Yes  \n",
       "16             Yes  \n",
       "17             Yes  \n",
       "18             Yes  \n",
       "19             Yes  \n",
       "20             Yes  \n",
       "21             Yes  \n",
       "22             Yes  \n",
       "23             Yes  \n",
       "24             Yes  \n",
       "25             Yes  \n",
       "26             Yes  \n",
       "27             Yes  \n",
       "28             Yes  \n",
       "29             Yes  \n",
       "..             ...  \n",
       "162            Yes  \n",
       "163            Yes  \n",
       "164            Yes  \n",
       "165            Yes  \n",
       "166            Yes  \n",
       "167            Yes  \n",
       "168            Yes  \n",
       "169            Yes  \n",
       "170            Yes  \n",
       "171            Yes  \n",
       "172            Yes  \n",
       "173            Yes  \n",
       "174            Yes  \n",
       "175            Yes  \n",
       "176            Yes  \n",
       "177            Yes  \n",
       "178            Yes  \n",
       "179            Yes  \n",
       "180            Yes  \n",
       "181            Yes  \n",
       "182            Yes  \n",
       "183            Yes  \n",
       "184            Yes  \n",
       "185            Yes  \n",
       "186            Yes  \n",
       "187            Yes  \n",
       "188            Yes  \n",
       "189            Yes  \n",
       "190            Yes  \n",
       "191            Yes  \n",
       "\n",
       "[192 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text you'd like to process is general-purpose English language text (i.e., not domain-specific, like medical literature), spaCy is ready to use out-of-the-box.\n",
    "\n",
    "I think it will eventually become a core part of the Python data science ecosystem &mdash; it will do for natural language computing what other great libraries have done for numerical computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Phrase modeling_ is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that _co-occur_ (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "\n",
    "...where:\n",
    "* $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "* $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "* $count(A\\ B)$ is the number of times the tokens $A\\ B$ appear in the corpus *in order*\n",
    "* $N$ is the total size of the corpus vocabulary\n",
    "* $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "* $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so _new york_ would become *new\\_york*). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as _happy hour_) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible [**gensim**](https://radimrehurek.com/gensim/index.html) library to help us with phrase modeling &mdash; the [**Phrases**](https://radimrehurek.com/gensim/models/phrases.html) class in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're performing phrase modeling, we'll be doing some iterative data transformation at the same time. Our roadmap for data preparation includes:\n",
    "\n",
    "1. Segment text of complete reviews into sentences & normalize text\n",
    "1. First-order phrase modeling $\\rightarrow$ _apply first-order phrase model to transform sentences_\n",
    "1. Second-order phrase modeling $\\rightarrow$ _apply second-order phrase model to transform sentences_\n",
    "1. Apply text normalization and second-order phrase model to text of complete reviews\n",
    "\n",
    "We'll use this transformed data as the input for some higher-level modeling approaches in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a few helper functions that we'll use for text normalization. In particular, the `lemmatized_sentence_corpus` generator function will use spaCy to:\n",
    "- Iterate over the 1M reviews in the `review_txt_all.txt` we created before\n",
    "- Segment the reviews into individual sentences\n",
    "- Remove punctuation and excess whitespace\n",
    "- Lemmatize the text\n",
    "\n",
    "... and do so efficiently in parallel, thanks to spaCy's `nlp.pipe()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `lemmatized_sentence_corpus` generator to loop over the original review text, segmenting the reviews into individual sentences and normalizing the text. We'll write this data back out to a new file (`unigram_sentences_all`), with one normalized sentence per line. We'll use this data for learning our phrase models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is organized like our `unigram_sentences_all` file now is &mdash; a large text file with one document/sentence per line &mdash; gensim's [**LineSentence**](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence) class provides a convenient iterator for working with other gensim components. It *streams* the documents/sentences from disk, so that you never have to hold the entire corpus in RAM at once. This allows you to scale your modeling pipeline up to potentially very large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few sample sentences in our new, transformed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food be great but with most newly open restaurant -PRON- still have something to work out\n",
      "\n",
      "-PRON- would definitely eat at sushi guru again to see if improve\n",
      "\n",
      "great atmospher great food great customer service\n",
      "\n",
      "finger lickin good\n",
      "\n",
      "will be back\n",
      "\n",
      "-PRON- be excited to go to this place because -PRON- have be hear good thing about -PRON-\n",
      "\n",
      "the restaurant have a really cool atmosphere and the staff be really nice\n",
      "\n",
      "although -PRON- feel like the menu be a bit small\n",
      "\n",
      "other than raman there be not much food and dessert\n",
      "\n",
      "basically -PRON- can customize -PRON- own raman\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print (u' '.join(unigram_sentence))\n",
    "    print (u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll learn a phrase model that will link individual words into two-word phrases. We'd expect words that together represent a specific concept, like \"`ice cream`\", to be linked together to form a new, single token: \"`ice_cream`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 815 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained phrase model for word pairs, let's apply it to the review sentences data and explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food be great but with most newly open restaurant -PRON- still have something to work out\n",
      "\n",
      "-PRON- would_definitely eat at sushi guru again to see if improve\n",
      "\n",
      "great atmospher great food great customer_service\n",
      "\n",
      "finger lickin good\n",
      "\n",
      "will be back\n",
      "\n",
      "-PRON- be excited to go to this_place because -PRON- have be hear good thing about -PRON-\n",
      "\n",
      "the restaurant have a really cool atmosphere and the staff be really nice\n",
      "\n",
      "although -PRON- feel_like the menu be a_bit small\n",
      "\n",
      "other_than raman there be not much food and dessert\n",
      "\n",
      "basically -PRON- can customize -PRON- own raman\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print (u' '.join(bigram_sentence))\n",
    "    print (u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the phrase modeling worked! We now see two-word phrases, such as \"`customer_service`\" and \"`feel_like`\", linked together in the text as a single token. Next, we'll train a _second-order_ phrase model. We'll apply the second-order phrase model on top of the already-transformed data, so that incomplete word combinations like \"`vanilla_ice cream`\" will become fully joined to \"`vanilla_ice_cream`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(intermediate_directory,\n",
    "                                      'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 766 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply our trained second-order phrase model to our first-order transformed sentences, write the results out to a new file, and explore a few of the second-order transformed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food be great but with most newly open restaurant -PRON- still have something to work out\n",
      "\n",
      "-PRON- would_definitely eat at sushi guru again to see if improve\n",
      "\n",
      "great atmospher great food great customer_service\n",
      "\n",
      "finger lickin good\n",
      "\n",
      "will be back\n",
      "\n",
      "-PRON- be excited to go to this_place because -PRON- have be hear good thing about -PRON-\n",
      "\n",
      "the restaurant have a really cool atmosphere and the staff be really nice\n",
      "\n",
      "although -PRON- feel_like the menu be a_bit small\n",
      "\n",
      "other_than raman there be not much food and dessert\n",
      "\n",
      "basically -PRON- can customize -PRON- own raman\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 230, 240):\n",
    "    print (u' '.join(trigram_sentence))\n",
    "    print (u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of our text preparation process circles back to the complete text of the reviews. We're going to run the complete text of the reviews through a pipeline that applies our text normalization and phrase models.\n",
    "\n",
    "In addition, we'll remove stopwords at this point. _Stopwords_ are very common words, like _a_, _the_, _and_, and so on, that serve functional roles in natural language, but typically don't contribute to the overall meaning of text. Filtering stopwords is a common procedure that allows higher-level NLP modeling techniques to focus on the words that carry more semantic weight.\n",
    "\n",
    "Finally, we'll write the transformed text out to a new file, with one review per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                        'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                      batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                              if term not in STOP_WORDS]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview the results. We'll grab one review from the file with the original, untransformed text, grab the same review from the file with the normalized and transformed text, and compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Not at all one of the best all you can eat sushi joints that I've been to. The food was okay at best. There was nothing special or standout about this place. They don't even have the Ipad's for ordering like a lot of the other places do. The menu was pretty basic and didn't really offer anything spectacular.\n",
      "\n",
      "It wasn't the worst and it wasn't the best. This was just a truly mediocre experience, I would go back if it was close by and if it was lunch time, but I would probably find somewhere else if I had to go for dinner since the price really isn't worth it.\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "good -PRON- eat sushi joint -PRON- food okay good nothing_special standout this_place -PRON- do_not_even ipad 's order like lot_of place menu pretty basic do_not offer spectacular -PRON- bad -PRON- good truly mediocre experience -PRON- go_back -PRON- close -PRON- lunch time -PRON- probably find somewhere_else -PRON- dinner price worth -PRON-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (u'Original:' + u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 11, 12):\n",
    "    print (review)\n",
    "\n",
    "print (u'----' + u'\\n')\n",
    "print (u'Transformed:' + u'\\n')\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 11, 12):\n",
    "        print (review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that most of the grammatical structure has been scrubbed from the text &mdash; capitalization, articles/conjunctions, punctuation, spacing, etc. However, much of the general semantic *meaning* is still present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop quiz! Can you complete this text snippet?\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word2vec quiz](https://s3.amazonaws.com/skipgram-images/word2vec-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "You just demonstrated the core machine learning concept behind word vector embedding models!\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word2vec quiz 2](https://s3.amazonaws.com/skipgram-images/word2vec-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised &mdash; they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n",
    "\n",
    "Perhaps the best-known word vector model is [word2vec](https://arxiv.org/pdf/1301.3781v3.pdf), originally proposed in 2013. The general idea of word2vec is, for a given *focus word*, to use the *context* of the word &mdash; i.e., the other words immediately before and after it &mdash; to provide hints about what the focus word might mean. To do this, word2vec uses a *sliding window* technique, where it considers snippets of text only a few tokens long at a time.\n",
    "\n",
    "At the start of the learning process, the model initializes random vectors for all terms in the corpus vocabulary. The model then slides the window across every snippet of text in the corpus, with each word taking turns as the focus word. Each time the model considers a new snippet, it tries to learn some information about the focus word based on the surrouding context, and it \"nudges\" the words' vector representations accordingly. One complete pass sliding the window across all of the corpus text is known as a training *epoch*. It's common to train a word2vec model for multiple passes/epochs over the corpus. Over time, the model rearranges the terms' vector representations such that terms that frequently appear in similar contexts have vector representations that are *close* to each other in vector space.\n",
    "\n",
    "For a deeper dive into word2vec's machine learning process, see [here](https://arxiv.org/pdf/1411.2738v4.pdf).\n",
    "\n",
    "Word2vec has a number of user-defined hyperparameters, including:\n",
    "- The dimensionality of the vectors. Typical choices include a few dozen to several hundred.\n",
    "- The width of the sliding window, in tokens. Five is a common default choice, but narrower and wider windows are possible.\n",
    "- The number of training epochs.\n",
    "\n",
    "For using word2vec in Python, [gensim](https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/) comes to the rescue again! It offers a [highly-optimized](https://rare-technologies.com/word2vec-in-python-part-two-optimizing/), [parallelized](https://rare-technologies.com/parallelizing-word2vec-in-python/) implementation of the word2vec algorithm with its [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our word2vec model using the normalized sentences with our phrase models applied. We'll use 100-dimensional vectors, and set up our training process to run for twelve epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:589: DeprecationWarning: Call to deprecated `cum_table` (Attribute will be removed in 4.0.0, use self.vocabulary.cum_table instead).\n",
      "  if hasattr(self, attrib):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:590: DeprecationWarning: Call to deprecated `cum_table` (Attribute will be removed in 4.0.0, use self.vocabulary.cum_table instead).\n",
      "  asides[attrib] = getattr(self, attrib)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:591: DeprecationWarning: Call to deprecated `cum_table` (Attribute will be removed in 4.0.0, use self.vocabulary.cum_table instead).\n",
      "  delattr(self, attrib)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:546: DeprecationWarning: Call to deprecated `cum_table` (Attribute will be removed in 4.0.0, use self.vocabulary.cum_table instead).\n",
      "  setattr(obj, attrib, val)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:488: DeprecationWarning: Call to deprecated `cum_table` (Attribute will be removed in 4.0.0, use self.vocabulary.cum_table instead).\n",
      "  setattr(self, attrib, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training epochs so far.\n",
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the word2vec model yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    # initiate the model and perform the first epoch of training\n",
    "    food2vec = Word2Vec(trigram_sentences, size=100, window=5,\n",
    "                        min_count=20, sg=1, workers=4)\n",
    "    \n",
    "    food2vec.save(word2vec_filepath)\n",
    "\n",
    "    #perform another 11 epochs of training\n",
    "    #for i in range(1,12):\n",
    "\n",
    "    #    food2vec.train(trigram_sentences,total_examples=1, epochs=10)\n",
    "    #    food2vec.save(word2vec_filepath)\n",
    "        \n",
    "# load the finished model from disk\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "food2vec.init_sims()\n",
    "\n",
    "print (u'{} training epochs so far.'.format(food2vec.train_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my four-core machine, each epoch over all the text in the ~1 million Yelp reviews takes about 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,024 terms in the food2vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print (u'{:,} terms in the food2vec vocabulary.'.format(len(food2vec.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the word vectors our model has learned. We'll create a pandas DataFrame with the terms as the row labels, and the 100 dimensions of the word vector model as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>-0.301643</td>\n",
       "      <td>0.039364</td>\n",
       "      <td>-0.145531</td>\n",
       "      <td>0.110742</td>\n",
       "      <td>0.144773</td>\n",
       "      <td>-0.078214</td>\n",
       "      <td>0.061256</td>\n",
       "      <td>-0.023227</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082084</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>0.110643</td>\n",
       "      <td>-0.116054</td>\n",
       "      <td>-0.063942</td>\n",
       "      <td>-0.063380</td>\n",
       "      <td>-0.021826</td>\n",
       "      <td>-0.042620</td>\n",
       "      <td>0.081347</td>\n",
       "      <td>-0.062028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>-0.280029</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>-0.076534</td>\n",
       "      <td>0.094662</td>\n",
       "      <td>0.101035</td>\n",
       "      <td>0.014097</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>-0.035879</td>\n",
       "      <td>0.128832</td>\n",
       "      <td>-0.055912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121852</td>\n",
       "      <td>0.024043</td>\n",
       "      <td>-0.013434</td>\n",
       "      <td>-0.132626</td>\n",
       "      <td>-0.118617</td>\n",
       "      <td>-0.072394</td>\n",
       "      <td>0.145242</td>\n",
       "      <td>-0.085593</td>\n",
       "      <td>0.092369</td>\n",
       "      <td>-0.071842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>-0.241954</td>\n",
       "      <td>0.111122</td>\n",
       "      <td>-0.103809</td>\n",
       "      <td>0.102062</td>\n",
       "      <td>0.054980</td>\n",
       "      <td>0.138506</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.088418</td>\n",
       "      <td>-0.011469</td>\n",
       "      <td>-0.066780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189196</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.064549</td>\n",
       "      <td>-0.073160</td>\n",
       "      <td>0.020430</td>\n",
       "      <td>-0.109981</td>\n",
       "      <td>0.084992</td>\n",
       "      <td>-0.042069</td>\n",
       "      <td>0.060030</td>\n",
       "      <td>0.068329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel_like</th>\n",
       "      <td>-0.255318</td>\n",
       "      <td>0.026057</td>\n",
       "      <td>-0.108719</td>\n",
       "      <td>0.105725</td>\n",
       "      <td>0.106940</td>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>0.039998</td>\n",
       "      <td>-0.036228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101847</td>\n",
       "      <td>-0.056427</td>\n",
       "      <td>0.073716</td>\n",
       "      <td>-0.163047</td>\n",
       "      <td>-0.051640</td>\n",
       "      <td>-0.047314</td>\n",
       "      <td>0.043506</td>\n",
       "      <td>-0.056412</td>\n",
       "      <td>0.143357</td>\n",
       "      <td>-0.030122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hubby</th>\n",
       "      <td>-0.243441</td>\n",
       "      <td>0.026121</td>\n",
       "      <td>-0.127511</td>\n",
       "      <td>0.090440</td>\n",
       "      <td>0.120251</td>\n",
       "      <td>0.048076</td>\n",
       "      <td>0.099165</td>\n",
       "      <td>0.073350</td>\n",
       "      <td>0.042066</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074756</td>\n",
       "      <td>-0.054829</td>\n",
       "      <td>0.132065</td>\n",
       "      <td>-0.169416</td>\n",
       "      <td>-0.020577</td>\n",
       "      <td>-0.020169</td>\n",
       "      <td>0.007734</td>\n",
       "      <td>-0.060025</td>\n",
       "      <td>0.147040</td>\n",
       "      <td>-0.029602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-PRON-</th>\n",
       "      <td>-0.172933</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>-0.015970</td>\n",
       "      <td>0.030818</td>\n",
       "      <td>0.108273</td>\n",
       "      <td>0.047040</td>\n",
       "      <td>0.026432</td>\n",
       "      <td>0.164874</td>\n",
       "      <td>-0.005151</td>\n",
       "      <td>0.047890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129939</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>-0.011828</td>\n",
       "      <td>-0.080609</td>\n",
       "      <td>0.093543</td>\n",
       "      <td>-0.004127</td>\n",
       "      <td>0.027260</td>\n",
       "      <td>-0.146688</td>\n",
       "      <td>0.106643</td>\n",
       "      <td>0.109947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decide_to</th>\n",
       "      <td>-0.302338</td>\n",
       "      <td>0.038833</td>\n",
       "      <td>-0.169464</td>\n",
       "      <td>0.111838</td>\n",
       "      <td>0.144900</td>\n",
       "      <td>-0.039691</td>\n",
       "      <td>0.091398</td>\n",
       "      <td>0.031585</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.008215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083595</td>\n",
       "      <td>-0.025650</td>\n",
       "      <td>0.136317</td>\n",
       "      <td>-0.132196</td>\n",
       "      <td>-0.035999</td>\n",
       "      <td>-0.046731</td>\n",
       "      <td>-0.024604</td>\n",
       "      <td>-0.076644</td>\n",
       "      <td>0.127589</td>\n",
       "      <td>-0.034433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grab</th>\n",
       "      <td>-0.276622</td>\n",
       "      <td>-0.020122</td>\n",
       "      <td>-0.077872</td>\n",
       "      <td>0.088111</td>\n",
       "      <td>0.125661</td>\n",
       "      <td>-0.082433</td>\n",
       "      <td>0.046129</td>\n",
       "      <td>0.024112</td>\n",
       "      <td>0.103239</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058610</td>\n",
       "      <td>-0.028942</td>\n",
       "      <td>0.124583</td>\n",
       "      <td>-0.091033</td>\n",
       "      <td>-0.150055</td>\n",
       "      <td>-0.085140</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>-0.028484</td>\n",
       "      <td>0.070502</td>\n",
       "      <td>-0.007932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drink</th>\n",
       "      <td>-0.265507</td>\n",
       "      <td>-0.019445</td>\n",
       "      <td>-0.028791</td>\n",
       "      <td>0.130656</td>\n",
       "      <td>0.066367</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>0.071866</td>\n",
       "      <td>0.056457</td>\n",
       "      <td>0.123166</td>\n",
       "      <td>-0.056270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093099</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>0.081738</td>\n",
       "      <td>-0.119529</td>\n",
       "      <td>-0.159087</td>\n",
       "      <td>-0.085912</td>\n",
       "      <td>-0.014769</td>\n",
       "      <td>0.014766</td>\n",
       "      <td>0.097616</td>\n",
       "      <td>0.051688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>-0.232834</td>\n",
       "      <td>-0.008028</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.045449</td>\n",
       "      <td>0.148151</td>\n",
       "      <td>-0.062544</td>\n",
       "      <td>-0.070685</td>\n",
       "      <td>-0.050062</td>\n",
       "      <td>-0.085577</td>\n",
       "      <td>-0.021800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116347</td>\n",
       "      <td>0.112865</td>\n",
       "      <td>0.015112</td>\n",
       "      <td>-0.122782</td>\n",
       "      <td>-0.061385</td>\n",
       "      <td>-0.003893</td>\n",
       "      <td>0.124158</td>\n",
       "      <td>-0.053636</td>\n",
       "      <td>0.158281</td>\n",
       "      <td>-0.181152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.217235</td>\n",
       "      <td>-0.091442</td>\n",
       "      <td>-0.079786</td>\n",
       "      <td>0.086086</td>\n",
       "      <td>0.128542</td>\n",
       "      <td>-0.099183</td>\n",
       "      <td>0.049231</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>-0.023959</td>\n",
       "      <td>0.038768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>-0.018015</td>\n",
       "      <td>0.057620</td>\n",
       "      <td>-0.220259</td>\n",
       "      <td>-0.051139</td>\n",
       "      <td>-0.048520</td>\n",
       "      <td>-0.034874</td>\n",
       "      <td>0.024243</td>\n",
       "      <td>0.236098</td>\n",
       "      <td>0.060681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bar</th>\n",
       "      <td>-0.182914</td>\n",
       "      <td>-0.077857</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.089116</td>\n",
       "      <td>0.096125</td>\n",
       "      <td>-0.043931</td>\n",
       "      <td>-0.050184</td>\n",
       "      <td>-0.063343</td>\n",
       "      <td>0.027625</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082654</td>\n",
       "      <td>0.058967</td>\n",
       "      <td>0.066999</td>\n",
       "      <td>-0.095022</td>\n",
       "      <td>-0.160072</td>\n",
       "      <td>-0.077366</td>\n",
       "      <td>0.021175</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.082698</td>\n",
       "      <td>-0.024255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>-0.076943</td>\n",
       "      <td>-0.043311</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.142384</td>\n",
       "      <td>-0.040853</td>\n",
       "      <td>0.094565</td>\n",
       "      <td>-0.096045</td>\n",
       "      <td>-0.069513</td>\n",
       "      <td>-0.020349</td>\n",
       "      <td>0.019036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>-0.006416</td>\n",
       "      <td>-0.092818</td>\n",
       "      <td>-0.112256</td>\n",
       "      <td>-0.005455</td>\n",
       "      <td>0.084190</td>\n",
       "      <td>0.080464</td>\n",
       "      <td>-0.046506</td>\n",
       "      <td>0.221583</td>\n",
       "      <td>-0.083481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>-0.264194</td>\n",
       "      <td>0.041895</td>\n",
       "      <td>-0.156831</td>\n",
       "      <td>0.067822</td>\n",
       "      <td>0.111014</td>\n",
       "      <td>0.081724</td>\n",
       "      <td>0.019833</td>\n",
       "      <td>0.055823</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>0.021427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083776</td>\n",
       "      <td>-0.032433</td>\n",
       "      <td>0.037823</td>\n",
       "      <td>-0.161740</td>\n",
       "      <td>0.106606</td>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.096896</td>\n",
       "      <td>-0.093088</td>\n",
       "      <td>0.107353</td>\n",
       "      <td>-0.076216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restaurant</th>\n",
       "      <td>-0.155358</td>\n",
       "      <td>0.013676</td>\n",
       "      <td>-0.037849</td>\n",
       "      <td>-0.020831</td>\n",
       "      <td>0.077404</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>-0.105253</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.030648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081639</td>\n",
       "      <td>0.006054</td>\n",
       "      <td>0.020035</td>\n",
       "      <td>-0.029016</td>\n",
       "      <td>-0.093303</td>\n",
       "      <td>-0.090172</td>\n",
       "      <td>0.103156</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>-0.013864</td>\n",
       "      <td>0.031861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>-0.250320</td>\n",
       "      <td>0.025583</td>\n",
       "      <td>-0.160300</td>\n",
       "      <td>0.090732</td>\n",
       "      <td>0.087693</td>\n",
       "      <td>0.092654</td>\n",
       "      <td>-0.097056</td>\n",
       "      <td>-0.021236</td>\n",
       "      <td>0.041902</td>\n",
       "      <td>-0.029398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140852</td>\n",
       "      <td>-0.083852</td>\n",
       "      <td>-0.030431</td>\n",
       "      <td>-0.141798</td>\n",
       "      <td>-0.016827</td>\n",
       "      <td>-0.053450</td>\n",
       "      <td>0.059146</td>\n",
       "      <td>-0.079585</td>\n",
       "      <td>0.112231</td>\n",
       "      <td>-0.042929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check_out</th>\n",
       "      <td>-0.263874</td>\n",
       "      <td>0.029739</td>\n",
       "      <td>-0.125860</td>\n",
       "      <td>0.073422</td>\n",
       "      <td>0.126357</td>\n",
       "      <td>0.038489</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096003</td>\n",
       "      <td>-0.051088</td>\n",
       "      <td>0.066583</td>\n",
       "      <td>-0.140750</td>\n",
       "      <td>-0.024504</td>\n",
       "      <td>-0.037287</td>\n",
       "      <td>0.039038</td>\n",
       "      <td>-0.090479</td>\n",
       "      <td>0.131652</td>\n",
       "      <td>-0.038950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>-0.124333</td>\n",
       "      <td>0.030990</td>\n",
       "      <td>-0.001277</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.036409</td>\n",
       "      <td>0.147466</td>\n",
       "      <td>-0.063234</td>\n",
       "      <td>-0.000724</td>\n",
       "      <td>0.017311</td>\n",
       "      <td>-0.079658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120157</td>\n",
       "      <td>-0.019734</td>\n",
       "      <td>-0.015272</td>\n",
       "      <td>-0.055105</td>\n",
       "      <td>-0.069816</td>\n",
       "      <td>-0.009967</td>\n",
       "      <td>0.156368</td>\n",
       "      <td>-0.084684</td>\n",
       "      <td>0.144807</td>\n",
       "      <td>-0.025015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>-0.185850</td>\n",
       "      <td>0.056456</td>\n",
       "      <td>-0.053573</td>\n",
       "      <td>0.095997</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>0.112520</td>\n",
       "      <td>-0.030319</td>\n",
       "      <td>0.035937</td>\n",
       "      <td>0.030135</td>\n",
       "      <td>-0.032415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133946</td>\n",
       "      <td>-0.030332</td>\n",
       "      <td>-0.003246</td>\n",
       "      <td>-0.216781</td>\n",
       "      <td>0.046137</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.109861</td>\n",
       "      <td>-0.098639</td>\n",
       "      <td>0.125218</td>\n",
       "      <td>-0.079987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>-0.148312</td>\n",
       "      <td>0.017234</td>\n",
       "      <td>-0.134600</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>0.037697</td>\n",
       "      <td>0.154111</td>\n",
       "      <td>-0.105876</td>\n",
       "      <td>0.016534</td>\n",
       "      <td>-0.024619</td>\n",
       "      <td>0.031548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126520</td>\n",
       "      <td>-0.080471</td>\n",
       "      <td>-0.120167</td>\n",
       "      <td>-0.148334</td>\n",
       "      <td>0.017786</td>\n",
       "      <td>0.036751</td>\n",
       "      <td>0.143493</td>\n",
       "      <td>-0.148049</td>\n",
       "      <td>-0.013735</td>\n",
       "      <td>-0.019713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>-0.159425</td>\n",
       "      <td>0.044309</td>\n",
       "      <td>-0.091575</td>\n",
       "      <td>0.110812</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>0.123958</td>\n",
       "      <td>-0.028790</td>\n",
       "      <td>-0.009200</td>\n",
       "      <td>0.038546</td>\n",
       "      <td>-0.083995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138924</td>\n",
       "      <td>-0.084110</td>\n",
       "      <td>0.023073</td>\n",
       "      <td>-0.152696</td>\n",
       "      <td>-0.018197</td>\n",
       "      <td>-0.023759</td>\n",
       "      <td>0.033978</td>\n",
       "      <td>-0.083228</td>\n",
       "      <td>0.154591</td>\n",
       "      <td>-0.014529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>up</th>\n",
       "      <td>-0.190384</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>-0.059373</td>\n",
       "      <td>0.128565</td>\n",
       "      <td>0.146280</td>\n",
       "      <td>-0.032321</td>\n",
       "      <td>0.064117</td>\n",
       "      <td>0.026622</td>\n",
       "      <td>-0.011645</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083954</td>\n",
       "      <td>0.058588</td>\n",
       "      <td>0.098408</td>\n",
       "      <td>-0.139288</td>\n",
       "      <td>-0.073390</td>\n",
       "      <td>-0.040055</td>\n",
       "      <td>-0.047843</td>\n",
       "      <td>-0.099835</td>\n",
       "      <td>0.161015</td>\n",
       "      <td>-0.049040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.119945</td>\n",
       "      <td>0.011105</td>\n",
       "      <td>-0.073061</td>\n",
       "      <td>0.104356</td>\n",
       "      <td>0.124572</td>\n",
       "      <td>-0.035959</td>\n",
       "      <td>-0.048099</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>0.073799</td>\n",
       "      <td>0.078263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217242</td>\n",
       "      <td>0.072346</td>\n",
       "      <td>0.089084</td>\n",
       "      <td>-0.129867</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>-0.109609</td>\n",
       "      <td>0.072611</td>\n",
       "      <td>-0.103029</td>\n",
       "      <td>0.123279</td>\n",
       "      <td>0.022389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>-0.102647</td>\n",
       "      <td>0.090519</td>\n",
       "      <td>-0.061616</td>\n",
       "      <td>0.082718</td>\n",
       "      <td>0.085153</td>\n",
       "      <td>0.093449</td>\n",
       "      <td>-0.029982</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>-0.018946</td>\n",
       "      <td>-0.061830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128063</td>\n",
       "      <td>-0.041702</td>\n",
       "      <td>0.041703</td>\n",
       "      <td>-0.146146</td>\n",
       "      <td>0.049008</td>\n",
       "      <td>0.007258</td>\n",
       "      <td>0.067190</td>\n",
       "      <td>-0.135276</td>\n",
       "      <td>0.194920</td>\n",
       "      <td>-0.081466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bartender</th>\n",
       "      <td>-0.163176</td>\n",
       "      <td>-0.065751</td>\n",
       "      <td>0.028665</td>\n",
       "      <td>0.142445</td>\n",
       "      <td>0.076216</td>\n",
       "      <td>0.018364</td>\n",
       "      <td>-0.096970</td>\n",
       "      <td>-0.018799</td>\n",
       "      <td>0.077756</td>\n",
       "      <td>-0.019417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141461</td>\n",
       "      <td>-0.010326</td>\n",
       "      <td>-0.023744</td>\n",
       "      <td>-0.136655</td>\n",
       "      <td>-0.077332</td>\n",
       "      <td>-0.080762</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.025731</td>\n",
       "      <td>0.134976</td>\n",
       "      <td>0.052527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extremely</th>\n",
       "      <td>-0.113996</td>\n",
       "      <td>-0.068890</td>\n",
       "      <td>0.031446</td>\n",
       "      <td>0.137442</td>\n",
       "      <td>0.042095</td>\n",
       "      <td>0.080367</td>\n",
       "      <td>-0.127166</td>\n",
       "      <td>-0.022503</td>\n",
       "      <td>0.095508</td>\n",
       "      <td>-0.044636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131820</td>\n",
       "      <td>-0.036635</td>\n",
       "      <td>-0.075845</td>\n",
       "      <td>-0.173771</td>\n",
       "      <td>-0.051308</td>\n",
       "      <td>-0.066063</td>\n",
       "      <td>0.046690</td>\n",
       "      <td>0.021176</td>\n",
       "      <td>0.166077</td>\n",
       "      <td>0.051639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friendly</th>\n",
       "      <td>-0.049328</td>\n",
       "      <td>-0.111236</td>\n",
       "      <td>0.101545</td>\n",
       "      <td>0.170209</td>\n",
       "      <td>0.021681</td>\n",
       "      <td>0.006229</td>\n",
       "      <td>-0.182176</td>\n",
       "      <td>-0.093966</td>\n",
       "      <td>0.076599</td>\n",
       "      <td>-0.037345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116309</td>\n",
       "      <td>-0.003496</td>\n",
       "      <td>-0.119352</td>\n",
       "      <td>-0.125601</td>\n",
       "      <td>-0.093192</td>\n",
       "      <td>-0.095697</td>\n",
       "      <td>-0.033788</td>\n",
       "      <td>0.053178</td>\n",
       "      <td>0.109140</td>\n",
       "      <td>0.082653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer</th>\n",
       "      <td>-0.199993</td>\n",
       "      <td>0.035591</td>\n",
       "      <td>-0.099651</td>\n",
       "      <td>0.046580</td>\n",
       "      <td>0.123776</td>\n",
       "      <td>0.049922</td>\n",
       "      <td>0.110275</td>\n",
       "      <td>0.044671</td>\n",
       "      <td>0.047642</td>\n",
       "      <td>-0.045501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049890</td>\n",
       "      <td>-0.108094</td>\n",
       "      <td>0.136606</td>\n",
       "      <td>-0.181013</td>\n",
       "      <td>-0.051054</td>\n",
       "      <td>0.015835</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>-0.045496</td>\n",
       "      <td>0.185183</td>\n",
       "      <td>-0.069647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>-0.268352</td>\n",
       "      <td>0.081486</td>\n",
       "      <td>-0.057553</td>\n",
       "      <td>0.083678</td>\n",
       "      <td>0.160143</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>-0.045749</td>\n",
       "      <td>-0.026483</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028110</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>0.031938</td>\n",
       "      <td>-0.166289</td>\n",
       "      <td>0.024168</td>\n",
       "      <td>0.012298</td>\n",
       "      <td>0.042775</td>\n",
       "      <td>-0.084110</td>\n",
       "      <td>0.169992</td>\n",
       "      <td>-0.141422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>try</th>\n",
       "      <td>-0.120649</td>\n",
       "      <td>0.053560</td>\n",
       "      <td>-0.183545</td>\n",
       "      <td>-0.089501</td>\n",
       "      <td>0.162069</td>\n",
       "      <td>0.031767</td>\n",
       "      <td>-0.011449</td>\n",
       "      <td>0.040253</td>\n",
       "      <td>-0.037094</td>\n",
       "      <td>0.027630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068346</td>\n",
       "      <td>-0.096059</td>\n",
       "      <td>0.018172</td>\n",
       "      <td>-0.047266</td>\n",
       "      <td>0.048249</td>\n",
       "      <td>0.030829</td>\n",
       "      <td>0.022094</td>\n",
       "      <td>-0.201801</td>\n",
       "      <td>0.104740</td>\n",
       "      <td>-0.087347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orange</th>\n",
       "      <td>-0.134568</td>\n",
       "      <td>-0.008101</td>\n",
       "      <td>-0.052164</td>\n",
       "      <td>0.081433</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>0.141167</td>\n",
       "      <td>0.159371</td>\n",
       "      <td>0.139769</td>\n",
       "      <td>0.097590</td>\n",
       "      <td>-0.020234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033744</td>\n",
       "      <td>-0.098006</td>\n",
       "      <td>0.126476</td>\n",
       "      <td>-0.234999</td>\n",
       "      <td>0.032690</td>\n",
       "      <td>0.025101</td>\n",
       "      <td>0.033042</td>\n",
       "      <td>-0.017234</td>\n",
       "      <td>0.196826</td>\n",
       "      <td>-0.021115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hot_dog</th>\n",
       "      <td>-0.199895</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>-0.091734</td>\n",
       "      <td>0.091460</td>\n",
       "      <td>0.093435</td>\n",
       "      <td>0.102883</td>\n",
       "      <td>0.069873</td>\n",
       "      <td>0.080582</td>\n",
       "      <td>0.065898</td>\n",
       "      <td>-0.024023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093061</td>\n",
       "      <td>-0.073875</td>\n",
       "      <td>0.091624</td>\n",
       "      <td>-0.207288</td>\n",
       "      <td>-0.014621</td>\n",
       "      <td>-0.012800</td>\n",
       "      <td>0.059202</td>\n",
       "      <td>-0.051539</td>\n",
       "      <td>0.188855</td>\n",
       "      <td>-0.025081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bun</th>\n",
       "      <td>-0.095203</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>-0.039305</td>\n",
       "      <td>0.058171</td>\n",
       "      <td>0.061237</td>\n",
       "      <td>0.185592</td>\n",
       "      <td>0.093531</td>\n",
       "      <td>0.090611</td>\n",
       "      <td>0.094581</td>\n",
       "      <td>-0.043389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066837</td>\n",
       "      <td>-0.112264</td>\n",
       "      <td>0.090474</td>\n",
       "      <td>-0.223939</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.031633</td>\n",
       "      <td>0.067059</td>\n",
       "      <td>-0.049272</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>-0.031476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explain</th>\n",
       "      <td>-0.225350</td>\n",
       "      <td>0.024843</td>\n",
       "      <td>-0.102920</td>\n",
       "      <td>0.086003</td>\n",
       "      <td>0.127228</td>\n",
       "      <td>0.025265</td>\n",
       "      <td>0.009211</td>\n",
       "      <td>-0.009770</td>\n",
       "      <td>0.017503</td>\n",
       "      <td>-0.024137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091812</td>\n",
       "      <td>-0.069140</td>\n",
       "      <td>0.058017</td>\n",
       "      <td>-0.153405</td>\n",
       "      <td>-0.039636</td>\n",
       "      <td>-0.027151</td>\n",
       "      <td>0.011393</td>\n",
       "      <td>-0.056738</td>\n",
       "      <td>0.154011</td>\n",
       "      <td>-0.046166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw</th>\n",
       "      <td>-0.155602</td>\n",
       "      <td>-0.009737</td>\n",
       "      <td>-0.046497</td>\n",
       "      <td>0.078742</td>\n",
       "      <td>0.076519</td>\n",
       "      <td>0.116957</td>\n",
       "      <td>0.128871</td>\n",
       "      <td>0.109669</td>\n",
       "      <td>0.108904</td>\n",
       "      <td>-0.011242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031632</td>\n",
       "      <td>-0.090107</td>\n",
       "      <td>0.125380</td>\n",
       "      <td>-0.227610</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.014986</td>\n",
       "      <td>0.048962</td>\n",
       "      <td>-0.010847</td>\n",
       "      <td>0.181290</td>\n",
       "      <td>-0.043763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>-0.269268</td>\n",
       "      <td>-0.009605</td>\n",
       "      <td>-0.104091</td>\n",
       "      <td>0.088365</td>\n",
       "      <td>0.126878</td>\n",
       "      <td>0.010743</td>\n",
       "      <td>0.109579</td>\n",
       "      <td>0.083621</td>\n",
       "      <td>0.067180</td>\n",
       "      <td>-0.001804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051207</td>\n",
       "      <td>-0.044133</td>\n",
       "      <td>0.148211</td>\n",
       "      <td>-0.153060</td>\n",
       "      <td>-0.074480</td>\n",
       "      <td>-0.047132</td>\n",
       "      <td>0.022512</td>\n",
       "      <td>-0.046828</td>\n",
       "      <td>0.117739</td>\n",
       "      <td>-0.023151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_time</th>\n",
       "      <td>-0.265877</td>\n",
       "      <td>0.019094</td>\n",
       "      <td>-0.113748</td>\n",
       "      <td>0.094383</td>\n",
       "      <td>0.137361</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.035809</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.062921</td>\n",
       "      <td>-0.007820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108582</td>\n",
       "      <td>-0.040959</td>\n",
       "      <td>0.067719</td>\n",
       "      <td>-0.131083</td>\n",
       "      <td>-0.059738</td>\n",
       "      <td>-0.060600</td>\n",
       "      <td>0.017728</td>\n",
       "      <td>-0.048727</td>\n",
       "      <td>0.119697</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pho</th>\n",
       "      <td>-0.201568</td>\n",
       "      <td>-0.008002</td>\n",
       "      <td>-0.104756</td>\n",
       "      <td>0.053234</td>\n",
       "      <td>0.079177</td>\n",
       "      <td>0.104995</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.052675</td>\n",
       "      <td>0.073493</td>\n",
       "      <td>-0.012183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086504</td>\n",
       "      <td>-0.085769</td>\n",
       "      <td>0.063588</td>\n",
       "      <td>-0.180522</td>\n",
       "      <td>-0.007674</td>\n",
       "      <td>-0.022085</td>\n",
       "      <td>0.116997</td>\n",
       "      <td>-0.059018</td>\n",
       "      <td>0.129633</td>\n",
       "      <td>-0.033448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>broth</th>\n",
       "      <td>-0.145643</td>\n",
       "      <td>-0.019689</td>\n",
       "      <td>-0.050678</td>\n",
       "      <td>0.087350</td>\n",
       "      <td>0.055972</td>\n",
       "      <td>0.149895</td>\n",
       "      <td>0.019091</td>\n",
       "      <td>0.069174</td>\n",
       "      <td>0.106688</td>\n",
       "      <td>-0.026523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083202</td>\n",
       "      <td>-0.101838</td>\n",
       "      <td>0.038775</td>\n",
       "      <td>-0.236579</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>-0.009415</td>\n",
       "      <td>0.087080</td>\n",
       "      <td>-0.038526</td>\n",
       "      <td>0.175840</td>\n",
       "      <td>-0.013471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expectation</th>\n",
       "      <td>-0.235396</td>\n",
       "      <td>0.034513</td>\n",
       "      <td>-0.123200</td>\n",
       "      <td>0.075882</td>\n",
       "      <td>0.097627</td>\n",
       "      <td>0.074660</td>\n",
       "      <td>-0.048446</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>-0.027695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111528</td>\n",
       "      <td>-0.070635</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>-0.159009</td>\n",
       "      <td>-0.009053</td>\n",
       "      <td>-0.033304</td>\n",
       "      <td>0.066230</td>\n",
       "      <td>-0.083825</td>\n",
       "      <td>0.149617</td>\n",
       "      <td>-0.040977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment</th>\n",
       "      <td>-0.258327</td>\n",
       "      <td>0.032122</td>\n",
       "      <td>-0.122335</td>\n",
       "      <td>0.079850</td>\n",
       "      <td>0.125781</td>\n",
       "      <td>0.040199</td>\n",
       "      <td>0.009753</td>\n",
       "      <td>0.016453</td>\n",
       "      <td>0.020552</td>\n",
       "      <td>-0.012439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095051</td>\n",
       "      <td>-0.048926</td>\n",
       "      <td>0.073279</td>\n",
       "      <td>-0.157980</td>\n",
       "      <td>-0.029615</td>\n",
       "      <td>-0.038201</td>\n",
       "      <td>0.044453</td>\n",
       "      <td>-0.051404</td>\n",
       "      <td>0.146514</td>\n",
       "      <td>-0.059833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>piece_of</th>\n",
       "      <td>-0.115687</td>\n",
       "      <td>-0.019912</td>\n",
       "      <td>-0.030063</td>\n",
       "      <td>0.071701</td>\n",
       "      <td>0.053188</td>\n",
       "      <td>0.140253</td>\n",
       "      <td>0.165934</td>\n",
       "      <td>0.132759</td>\n",
       "      <td>0.109562</td>\n",
       "      <td>-0.019658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>-0.096120</td>\n",
       "      <td>0.132523</td>\n",
       "      <td>-0.236465</td>\n",
       "      <td>0.021174</td>\n",
       "      <td>0.035524</td>\n",
       "      <td>0.045692</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.197716</td>\n",
       "      <td>-0.031886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invite</th>\n",
       "      <td>-0.220053</td>\n",
       "      <td>-0.031038</td>\n",
       "      <td>-0.071240</td>\n",
       "      <td>0.112423</td>\n",
       "      <td>0.095032</td>\n",
       "      <td>0.058652</td>\n",
       "      <td>-0.054314</td>\n",
       "      <td>-0.001097</td>\n",
       "      <td>0.081677</td>\n",
       "      <td>-0.030122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110625</td>\n",
       "      <td>-0.072517</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>-0.176168</td>\n",
       "      <td>-0.060853</td>\n",
       "      <td>-0.057147</td>\n",
       "      <td>0.044378</td>\n",
       "      <td>-0.014952</td>\n",
       "      <td>0.153506</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chair</th>\n",
       "      <td>-0.219249</td>\n",
       "      <td>-0.008653</td>\n",
       "      <td>-0.076660</td>\n",
       "      <td>0.112054</td>\n",
       "      <td>0.101098</td>\n",
       "      <td>0.063385</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>0.047147</td>\n",
       "      <td>-0.022612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102268</td>\n",
       "      <td>-0.052350</td>\n",
       "      <td>0.061911</td>\n",
       "      <td>-0.186854</td>\n",
       "      <td>-0.038936</td>\n",
       "      <td>-0.036635</td>\n",
       "      <td>0.034621</td>\n",
       "      <td>-0.047665</td>\n",
       "      <td>0.152779</td>\n",
       "      <td>-0.044512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king</th>\n",
       "      <td>-0.240810</td>\n",
       "      <td>-0.002109</td>\n",
       "      <td>-0.103501</td>\n",
       "      <td>0.084984</td>\n",
       "      <td>0.116373</td>\n",
       "      <td>0.067022</td>\n",
       "      <td>0.054343</td>\n",
       "      <td>0.056927</td>\n",
       "      <td>0.055250</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079932</td>\n",
       "      <td>-0.065018</td>\n",
       "      <td>0.099747</td>\n",
       "      <td>-0.191130</td>\n",
       "      <td>-0.021468</td>\n",
       "      <td>-0.034669</td>\n",
       "      <td>0.053461</td>\n",
       "      <td>-0.040530</td>\n",
       "      <td>0.157559</td>\n",
       "      <td>-0.039047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parking</th>\n",
       "      <td>-0.287813</td>\n",
       "      <td>-0.016635</td>\n",
       "      <td>-0.125254</td>\n",
       "      <td>0.091501</td>\n",
       "      <td>0.120021</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.058541</td>\n",
       "      <td>0.024453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084097</td>\n",
       "      <td>-0.048379</td>\n",
       "      <td>0.075182</td>\n",
       "      <td>-0.149978</td>\n",
       "      <td>-0.059116</td>\n",
       "      <td>-0.068502</td>\n",
       "      <td>0.056446</td>\n",
       "      <td>-0.055794</td>\n",
       "      <td>0.063947</td>\n",
       "      <td>-0.030707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>-0.237587</td>\n",
       "      <td>0.011639</td>\n",
       "      <td>-0.108190</td>\n",
       "      <td>0.099484</td>\n",
       "      <td>0.114466</td>\n",
       "      <td>0.073378</td>\n",
       "      <td>0.008052</td>\n",
       "      <td>0.026023</td>\n",
       "      <td>0.047370</td>\n",
       "      <td>-0.012905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098315</td>\n",
       "      <td>-0.060796</td>\n",
       "      <td>0.055035</td>\n",
       "      <td>-0.180568</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-0.027469</td>\n",
       "      <td>0.064162</td>\n",
       "      <td>-0.067593</td>\n",
       "      <td>0.164537</td>\n",
       "      <td>-0.039014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ice_cream</th>\n",
       "      <td>-0.167863</td>\n",
       "      <td>-0.006100</td>\n",
       "      <td>-0.057198</td>\n",
       "      <td>0.082054</td>\n",
       "      <td>0.078843</td>\n",
       "      <td>0.126532</td>\n",
       "      <td>0.055984</td>\n",
       "      <td>0.088904</td>\n",
       "      <td>0.083470</td>\n",
       "      <td>-0.027493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070473</td>\n",
       "      <td>-0.083011</td>\n",
       "      <td>0.070246</td>\n",
       "      <td>-0.219129</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.064245</td>\n",
       "      <td>-0.022119</td>\n",
       "      <td>0.198504</td>\n",
       "      <td>-0.030411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locate_in</th>\n",
       "      <td>-0.237335</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>-0.113809</td>\n",
       "      <td>0.074387</td>\n",
       "      <td>0.115941</td>\n",
       "      <td>0.049212</td>\n",
       "      <td>-0.010928</td>\n",
       "      <td>-0.003094</td>\n",
       "      <td>0.068173</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094252</td>\n",
       "      <td>-0.073498</td>\n",
       "      <td>0.051965</td>\n",
       "      <td>-0.178842</td>\n",
       "      <td>-0.039867</td>\n",
       "      <td>-0.031053</td>\n",
       "      <td>0.074643</td>\n",
       "      <td>-0.056070</td>\n",
       "      <td>0.131874</td>\n",
       "      <td>-0.064682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fast_food</th>\n",
       "      <td>-0.251098</td>\n",
       "      <td>-0.031444</td>\n",
       "      <td>-0.076473</td>\n",
       "      <td>0.088338</td>\n",
       "      <td>0.097156</td>\n",
       "      <td>0.048508</td>\n",
       "      <td>-0.016638</td>\n",
       "      <td>0.041001</td>\n",
       "      <td>0.089873</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077125</td>\n",
       "      <td>-0.052160</td>\n",
       "      <td>0.062297</td>\n",
       "      <td>-0.164139</td>\n",
       "      <td>-0.053399</td>\n",
       "      <td>-0.059141</td>\n",
       "      <td>0.086266</td>\n",
       "      <td>-0.042504</td>\n",
       "      <td>0.126710</td>\n",
       "      <td>-0.027194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simply</th>\n",
       "      <td>-0.203782</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.078228</td>\n",
       "      <td>0.088007</td>\n",
       "      <td>0.079543</td>\n",
       "      <td>0.106738</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>0.067020</td>\n",
       "      <td>0.083927</td>\n",
       "      <td>-0.022573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086801</td>\n",
       "      <td>-0.072506</td>\n",
       "      <td>0.057031</td>\n",
       "      <td>-0.197492</td>\n",
       "      <td>-0.011134</td>\n",
       "      <td>-0.023997</td>\n",
       "      <td>0.063496</td>\n",
       "      <td>-0.040919</td>\n",
       "      <td>0.175969</td>\n",
       "      <td>-0.015342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>-0.204011</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>-0.087217</td>\n",
       "      <td>0.088732</td>\n",
       "      <td>0.103674</td>\n",
       "      <td>0.099245</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>0.069204</td>\n",
       "      <td>0.062069</td>\n",
       "      <td>-0.014024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098396</td>\n",
       "      <td>-0.071452</td>\n",
       "      <td>0.079557</td>\n",
       "      <td>-0.211514</td>\n",
       "      <td>-0.001717</td>\n",
       "      <td>-0.014139</td>\n",
       "      <td>0.044258</td>\n",
       "      <td>-0.057092</td>\n",
       "      <td>0.163729</td>\n",
       "      <td>-0.025052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obviously</th>\n",
       "      <td>-0.271702</td>\n",
       "      <td>0.017655</td>\n",
       "      <td>-0.139977</td>\n",
       "      <td>0.094990</td>\n",
       "      <td>0.120671</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>-0.030200</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.008351</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122923</td>\n",
       "      <td>-0.037511</td>\n",
       "      <td>0.034547</td>\n",
       "      <td>-0.131934</td>\n",
       "      <td>-0.027970</td>\n",
       "      <td>-0.069343</td>\n",
       "      <td>0.032292</td>\n",
       "      <td>-0.081633</td>\n",
       "      <td>0.102622</td>\n",
       "      <td>-0.015472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample</th>\n",
       "      <td>-0.220338</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>-0.085298</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.117913</td>\n",
       "      <td>0.058444</td>\n",
       "      <td>0.083022</td>\n",
       "      <td>0.048697</td>\n",
       "      <td>0.063006</td>\n",
       "      <td>-0.023321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068327</td>\n",
       "      <td>-0.071432</td>\n",
       "      <td>0.104777</td>\n",
       "      <td>-0.201297</td>\n",
       "      <td>-0.034264</td>\n",
       "      <td>-0.012804</td>\n",
       "      <td>0.029755</td>\n",
       "      <td>-0.049235</td>\n",
       "      <td>0.181356</td>\n",
       "      <td>-0.050868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thai</th>\n",
       "      <td>-0.189943</td>\n",
       "      <td>-0.043958</td>\n",
       "      <td>-0.093067</td>\n",
       "      <td>0.041062</td>\n",
       "      <td>0.065005</td>\n",
       "      <td>0.098824</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.099933</td>\n",
       "      <td>0.093091</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063512</td>\n",
       "      <td>-0.087092</td>\n",
       "      <td>0.065034</td>\n",
       "      <td>-0.187905</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>-0.029168</td>\n",
       "      <td>0.124098</td>\n",
       "      <td>-0.031383</td>\n",
       "      <td>0.143490</td>\n",
       "      <td>0.004485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delish</th>\n",
       "      <td>-0.188054</td>\n",
       "      <td>-0.013136</td>\n",
       "      <td>-0.074835</td>\n",
       "      <td>0.099054</td>\n",
       "      <td>0.085707</td>\n",
       "      <td>0.118189</td>\n",
       "      <td>0.016722</td>\n",
       "      <td>0.056340</td>\n",
       "      <td>0.073341</td>\n",
       "      <td>-0.029567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102618</td>\n",
       "      <td>-0.080999</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>-0.216387</td>\n",
       "      <td>-0.007737</td>\n",
       "      <td>-0.022385</td>\n",
       "      <td>0.060625</td>\n",
       "      <td>-0.046491</td>\n",
       "      <td>0.168931</td>\n",
       "      <td>-0.011795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ball</th>\n",
       "      <td>-0.192272</td>\n",
       "      <td>0.008669</td>\n",
       "      <td>-0.098868</td>\n",
       "      <td>0.061325</td>\n",
       "      <td>0.097176</td>\n",
       "      <td>0.112631</td>\n",
       "      <td>0.084272</td>\n",
       "      <td>0.106991</td>\n",
       "      <td>0.073036</td>\n",
       "      <td>-0.005957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063003</td>\n",
       "      <td>-0.098353</td>\n",
       "      <td>0.106936</td>\n",
       "      <td>-0.207373</td>\n",
       "      <td>0.010371</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.046792</td>\n",
       "      <td>-0.035770</td>\n",
       "      <td>0.157458</td>\n",
       "      <td>-0.042737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagel</th>\n",
       "      <td>-0.203474</td>\n",
       "      <td>0.012465</td>\n",
       "      <td>-0.095682</td>\n",
       "      <td>0.082785</td>\n",
       "      <td>0.084156</td>\n",
       "      <td>0.111309</td>\n",
       "      <td>0.065659</td>\n",
       "      <td>0.088904</td>\n",
       "      <td>0.073314</td>\n",
       "      <td>-0.026709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089134</td>\n",
       "      <td>-0.087373</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>-0.197255</td>\n",
       "      <td>-0.005157</td>\n",
       "      <td>-0.004948</td>\n",
       "      <td>0.067762</td>\n",
       "      <td>-0.046329</td>\n",
       "      <td>0.171968</td>\n",
       "      <td>-0.024236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican</th>\n",
       "      <td>-0.216053</td>\n",
       "      <td>0.018394</td>\n",
       "      <td>-0.108839</td>\n",
       "      <td>0.065133</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.091035</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>0.051812</td>\n",
       "      <td>0.059844</td>\n",
       "      <td>-0.016843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099312</td>\n",
       "      <td>-0.079592</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>-0.180331</td>\n",
       "      <td>-0.013603</td>\n",
       "      <td>-0.024326</td>\n",
       "      <td>0.093638</td>\n",
       "      <td>-0.061941</td>\n",
       "      <td>0.162244</td>\n",
       "      <td>-0.035284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rock</th>\n",
       "      <td>-0.176925</td>\n",
       "      <td>-0.007574</td>\n",
       "      <td>-0.057891</td>\n",
       "      <td>0.080107</td>\n",
       "      <td>0.097086</td>\n",
       "      <td>0.105276</td>\n",
       "      <td>0.093621</td>\n",
       "      <td>0.078305</td>\n",
       "      <td>0.073366</td>\n",
       "      <td>-0.018566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058398</td>\n",
       "      <td>-0.061515</td>\n",
       "      <td>0.110698</td>\n",
       "      <td>-0.216987</td>\n",
       "      <td>-0.009525</td>\n",
       "      <td>0.006905</td>\n",
       "      <td>0.059736</td>\n",
       "      <td>-0.031415</td>\n",
       "      <td>0.202909</td>\n",
       "      <td>-0.048059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5   \\\n",
       "after       -0.301643  0.039364 -0.145531  0.110742  0.144773 -0.078214   \n",
       "for         -0.280029  0.024631 -0.076534  0.094662  0.101035  0.014097   \n",
       "what        -0.241954  0.111122 -0.103809  0.102062  0.054980  0.138506   \n",
       "feel_like   -0.255318  0.026057 -0.108719  0.105725  0.106940  0.057198   \n",
       "hubby       -0.243441  0.026121 -0.127511  0.090440  0.120251  0.048076   \n",
       "-PRON-      -0.172933  0.008653 -0.015970  0.030818  0.108273  0.047040   \n",
       "decide_to   -0.302338  0.038833 -0.169464  0.111838  0.144900 -0.039691   \n",
       "grab        -0.276622 -0.020122 -0.077872  0.088111  0.125661 -0.082433   \n",
       "drink       -0.265507 -0.019445 -0.028791  0.130656  0.066367  0.002516   \n",
       "at          -0.232834 -0.008028  0.000442  0.045449  0.148151 -0.062544   \n",
       "the         -0.217235 -0.091442 -0.079786  0.086086  0.128542 -0.099183   \n",
       "bar         -0.182914 -0.077857  0.005302  0.089116  0.096125 -0.043931   \n",
       "be          -0.076943 -0.043311  0.063830  0.142384 -0.040853  0.094565   \n",
       "that        -0.264194  0.041895 -0.156831  0.067822  0.111014  0.081724   \n",
       "restaurant  -0.155358  0.013676 -0.037849 -0.020831  0.077404 -0.012037   \n",
       "worth       -0.250320  0.025583 -0.160300  0.090732  0.087693  0.092654   \n",
       "check_out   -0.263874  0.029739 -0.125860  0.073422  0.126357  0.038489   \n",
       "but         -0.124333  0.030990 -0.001277  0.004492  0.036409  0.147466   \n",
       "just        -0.185850  0.056456 -0.053573  0.095997  0.015316  0.112520   \n",
       "not         -0.148312  0.017234 -0.134600  0.012483  0.037697  0.154111   \n",
       "feel        -0.159425  0.044309 -0.091575  0.110812  0.084509  0.123958   \n",
       "up          -0.190384  0.002962 -0.059373  0.128565  0.146280 -0.032321   \n",
       "to          -0.119945  0.011105 -0.073061  0.104356  0.124572 -0.035959   \n",
       "do          -0.102647  0.090519 -0.061616  0.082718  0.085153  0.093449   \n",
       "bartender   -0.163176 -0.065751  0.028665  0.142445  0.076216  0.018364   \n",
       "extremely   -0.113996 -0.068890  0.031446  0.137442  0.042095  0.080367   \n",
       "friendly    -0.049328 -0.111236  0.101545  0.170209  0.021681  0.006229   \n",
       "offer       -0.199993  0.035591 -0.099651  0.046580  0.123776  0.049922   \n",
       "about       -0.268352  0.081486 -0.057553  0.083678  0.160143 -0.005266   \n",
       "try         -0.120649  0.053560 -0.183545 -0.089501  0.162069  0.031767   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "orange      -0.134568 -0.008101 -0.052164  0.081433  0.067281  0.141167   \n",
       "hot_dog     -0.199895  0.008707 -0.091734  0.091460  0.093435  0.102883   \n",
       "bun         -0.095203  0.012190 -0.039305  0.058171  0.061237  0.185592   \n",
       "explain     -0.225350  0.024843 -0.102920  0.086003  0.127228  0.025265   \n",
       "raw         -0.155602 -0.009737 -0.046497  0.078742  0.076519  0.116957   \n",
       "date        -0.269268 -0.009605 -0.104091  0.088365  0.126878  0.010743   \n",
       "long_time   -0.265877  0.019094 -0.113748  0.094383  0.137361  0.010258   \n",
       "pho         -0.201568 -0.008002 -0.104756  0.053234  0.079177  0.104995   \n",
       "broth       -0.145643 -0.019689 -0.050678  0.087350  0.055972  0.149895   \n",
       "expectation -0.235396  0.034513 -0.123200  0.075882  0.097627  0.074660   \n",
       "comment     -0.258327  0.032122 -0.122335  0.079850  0.125781  0.040199   \n",
       "piece_of    -0.115687 -0.019912 -0.030063  0.071701  0.053188  0.140253   \n",
       "invite      -0.220053 -0.031038 -0.071240  0.112423  0.095032  0.058652   \n",
       "chair       -0.219249 -0.008653 -0.076660  0.112054  0.101098  0.063385   \n",
       "king        -0.240810 -0.002109 -0.103501  0.084984  0.116373  0.067022   \n",
       "parking     -0.287813 -0.016635 -0.125254  0.091501  0.120021 -0.000613   \n",
       "yeah        -0.237587  0.011639 -0.108190  0.099484  0.114466  0.073378   \n",
       "ice_cream   -0.167863 -0.006100 -0.057198  0.082054  0.078843  0.126532   \n",
       "locate_in   -0.237335  0.005784 -0.113809  0.074387  0.115941  0.049212   \n",
       "fast_food   -0.251098 -0.031444 -0.076473  0.088338  0.097156  0.048508   \n",
       "simply      -0.203782 -0.006899 -0.078228  0.088007  0.079543  0.106738   \n",
       "double      -0.204011  0.008060 -0.087217  0.088732  0.103674  0.099245   \n",
       "obviously   -0.271702  0.017655 -0.139977  0.094990  0.120671  0.007792   \n",
       "sample      -0.220338  0.001236 -0.085298  0.091412  0.117913  0.058444   \n",
       "thai        -0.189943 -0.043958 -0.093067  0.041062  0.065005  0.098824   \n",
       "delish      -0.188054 -0.013136 -0.074835  0.099054  0.085707  0.118189   \n",
       "ball        -0.192272  0.008669 -0.098868  0.061325  0.097176  0.112631   \n",
       "bagel       -0.203474  0.012465 -0.095682  0.082785  0.084156  0.111309   \n",
       "mexican     -0.216053  0.018394 -0.108839  0.065133  0.106592  0.091035   \n",
       "rock        -0.176925 -0.007574 -0.057891  0.080107  0.097086  0.105276   \n",
       "\n",
       "                   6         7         8         9     ...           90  \\\n",
       "after        0.061256 -0.023227  0.003406  0.011680    ...     0.082084   \n",
       "for          0.002313 -0.035879  0.128832 -0.055912    ...     0.121852   \n",
       "what         0.001434  0.088418 -0.011469 -0.066780    ...     0.189196   \n",
       "feel_like    0.029765  0.011120  0.039998 -0.036228    ...     0.101847   \n",
       "hubby        0.099165  0.073350  0.042066  0.000108    ...     0.074756   \n",
       "-PRON-       0.026432  0.164874 -0.005151  0.047890    ...     0.129939   \n",
       "decide_to    0.091398  0.031585  0.001609  0.008215    ...     0.083595   \n",
       "grab         0.046129  0.024112  0.103239  0.001841    ...     0.058610   \n",
       "drink        0.071866  0.056457  0.123166 -0.056270    ...     0.093099   \n",
       "at          -0.070685 -0.050062 -0.085577 -0.021800    ...     0.116347   \n",
       "the          0.049231  0.062772 -0.023959  0.038768    ...     0.012205   \n",
       "bar         -0.050184 -0.063343  0.027625  0.009606    ...     0.082654   \n",
       "be          -0.096045 -0.069513 -0.020349  0.019036    ...     0.002820   \n",
       "that         0.019833  0.055823 -0.002075  0.021427    ...     0.083776   \n",
       "restaurant  -0.105253 -0.043307 -0.000552  0.030648    ...     0.081639   \n",
       "worth       -0.097056 -0.021236  0.041902 -0.029398    ...     0.140852   \n",
       "check_out   -0.000764  0.025890  0.017313  0.003599    ...     0.096003   \n",
       "but         -0.063234 -0.000724  0.017311 -0.079658    ...     0.120157   \n",
       "just        -0.030319  0.035937  0.030135 -0.032415    ...     0.133946   \n",
       "not         -0.105876  0.016534 -0.024619  0.031548    ...     0.126520   \n",
       "feel        -0.028790 -0.009200  0.038546 -0.083995    ...     0.138924   \n",
       "up           0.064117  0.026622 -0.011645  0.004206    ...     0.083954   \n",
       "to          -0.048099  0.046402  0.073799  0.078263    ...     0.217242   \n",
       "do          -0.029982  0.007726 -0.018946 -0.061830    ...     0.128063   \n",
       "bartender   -0.096970 -0.018799  0.077756 -0.019417    ...     0.141461   \n",
       "extremely   -0.127166 -0.022503  0.095508 -0.044636    ...     0.131820   \n",
       "friendly    -0.182176 -0.093966  0.076599 -0.037345    ...     0.116309   \n",
       "offer        0.110275  0.044671  0.047642 -0.045501    ...     0.049890   \n",
       "about        0.005619 -0.045749 -0.026483  0.000674    ...     0.028110   \n",
       "try         -0.011449  0.040253 -0.037094  0.027630    ...     0.068346   \n",
       "...               ...       ...       ...       ...    ...          ...   \n",
       "orange       0.159371  0.139769  0.097590 -0.020234    ...     0.033744   \n",
       "hot_dog      0.069873  0.080582  0.065898 -0.024023    ...     0.093061   \n",
       "bun          0.093531  0.090611  0.094581 -0.043389    ...     0.066837   \n",
       "explain      0.009211 -0.009770  0.017503 -0.024137    ...     0.091812   \n",
       "raw          0.128871  0.109669  0.108904 -0.011242    ...     0.031632   \n",
       "date         0.109579  0.083621  0.067180 -0.001804    ...     0.051207   \n",
       "long_time    0.035809  0.019714  0.062921 -0.007820    ...     0.108582   \n",
       "pho          0.001749  0.052675  0.073493 -0.012183    ...     0.086504   \n",
       "broth        0.019091  0.069174  0.106688 -0.026523    ...     0.083202   \n",
       "expectation -0.048446  0.005595  0.013295 -0.027695    ...     0.111528   \n",
       "comment      0.009753  0.016453  0.020552 -0.012439    ...     0.095051   \n",
       "piece_of     0.165934  0.132759  0.109562 -0.019658    ...     0.007494   \n",
       "invite      -0.054314 -0.001097  0.081677 -0.030122    ...     0.110625   \n",
       "chair        0.016530  0.008188  0.047147 -0.022612    ...     0.102268   \n",
       "king         0.054343  0.056927  0.055250 -0.000761    ...     0.079932   \n",
       "parking      0.005597  0.011083  0.058541  0.024453    ...     0.084097   \n",
       "yeah         0.008052  0.026023  0.047370 -0.012905    ...     0.098315   \n",
       "ice_cream    0.055984  0.088904  0.083470 -0.027493    ...     0.070473   \n",
       "locate_in   -0.010928 -0.003094  0.068173  0.000739    ...     0.094252   \n",
       "fast_food   -0.016638  0.041001  0.089873  0.002966    ...     0.077125   \n",
       "simply       0.029031  0.067020  0.083927 -0.022573    ...     0.086801   \n",
       "double       0.060199  0.069204  0.062069 -0.014024    ...     0.098396   \n",
       "obviously   -0.030200  0.003972  0.008351  0.006349    ...     0.122923   \n",
       "sample       0.083022  0.048697  0.063006 -0.023321    ...     0.068327   \n",
       "thai         0.000577  0.099933  0.093091  0.002195    ...     0.063512   \n",
       "delish       0.016722  0.056340  0.073341 -0.029567    ...     0.102618   \n",
       "ball         0.084272  0.106991  0.073036 -0.005957    ...     0.063003   \n",
       "bagel        0.065659  0.088904  0.073314 -0.026709    ...     0.089134   \n",
       "mexican      0.005282  0.051812  0.059844 -0.016843    ...     0.099312   \n",
       "rock         0.093621  0.078305  0.073366 -0.018566    ...     0.058398   \n",
       "\n",
       "                   91        92        93        94        95        96  \\\n",
       "after        0.014832  0.110643 -0.116054 -0.063942 -0.063380 -0.021826   \n",
       "for          0.024043 -0.013434 -0.132626 -0.118617 -0.072394  0.145242   \n",
       "what         0.029457  0.064549 -0.073160  0.020430 -0.109981  0.084992   \n",
       "feel_like   -0.056427  0.073716 -0.163047 -0.051640 -0.047314  0.043506   \n",
       "hubby       -0.054829  0.132065 -0.169416 -0.020577 -0.020169  0.007734   \n",
       "-PRON-       0.020924 -0.011828 -0.080609  0.093543 -0.004127  0.027260   \n",
       "decide_to   -0.025650  0.136317 -0.132196 -0.035999 -0.046731 -0.024604   \n",
       "grab        -0.028942  0.124583 -0.091033 -0.150055 -0.085140  0.019366   \n",
       "drink        0.015193  0.081738 -0.119529 -0.159087 -0.085912 -0.014769   \n",
       "at           0.112865  0.015112 -0.122782 -0.061385 -0.003893  0.124158   \n",
       "the         -0.018015  0.057620 -0.220259 -0.051139 -0.048520 -0.034874   \n",
       "bar          0.058967  0.066999 -0.095022 -0.160072 -0.077366  0.021175   \n",
       "be          -0.006416 -0.092818 -0.112256 -0.005455  0.084190  0.080464   \n",
       "that        -0.032433  0.037823 -0.161740  0.106606  0.010851  0.096896   \n",
       "restaurant   0.006054  0.020035 -0.029016 -0.093303 -0.090172  0.103156   \n",
       "worth       -0.083852 -0.030431 -0.141798 -0.016827 -0.053450  0.059146   \n",
       "check_out   -0.051088  0.066583 -0.140750 -0.024504 -0.037287  0.039038   \n",
       "but         -0.019734 -0.015272 -0.055105 -0.069816 -0.009967  0.156368   \n",
       "just        -0.030332 -0.003246 -0.216781  0.046137  0.001597  0.109861   \n",
       "not         -0.080471 -0.120167 -0.148334  0.017786  0.036751  0.143493   \n",
       "feel        -0.084110  0.023073 -0.152696 -0.018197 -0.023759  0.033978   \n",
       "up           0.058588  0.098408 -0.139288 -0.073390 -0.040055 -0.047843   \n",
       "to           0.072346  0.089084 -0.129867  0.005339 -0.109609  0.072611   \n",
       "do          -0.041702  0.041703 -0.146146  0.049008  0.007258  0.067190   \n",
       "bartender   -0.010326 -0.023744 -0.136655 -0.077332 -0.080762  0.009091   \n",
       "extremely   -0.036635 -0.075845 -0.173771 -0.051308 -0.066063  0.046690   \n",
       "friendly    -0.003496 -0.119352 -0.125601 -0.093192 -0.095697 -0.033788   \n",
       "offer       -0.108094  0.136606 -0.181013 -0.051054  0.015835  0.013138   \n",
       "about       -0.001580  0.031938 -0.166289  0.024168  0.012298  0.042775   \n",
       "try         -0.096059  0.018172 -0.047266  0.048249  0.030829  0.022094   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "orange      -0.098006  0.126476 -0.234999  0.032690  0.025101  0.033042   \n",
       "hot_dog     -0.073875  0.091624 -0.207288 -0.014621 -0.012800  0.059202   \n",
       "bun         -0.112264  0.090474 -0.223939  0.018947  0.031633  0.067059   \n",
       "explain     -0.069140  0.058017 -0.153405 -0.039636 -0.027151  0.011393   \n",
       "raw         -0.090107  0.125380 -0.227610  0.004676  0.014986  0.048962   \n",
       "date        -0.044133  0.148211 -0.153060 -0.074480 -0.047132  0.022512   \n",
       "long_time   -0.040959  0.067719 -0.131083 -0.059738 -0.060600  0.017728   \n",
       "pho         -0.085769  0.063588 -0.180522 -0.007674 -0.022085  0.116997   \n",
       "broth       -0.101838  0.038775 -0.236579  0.021850 -0.009415  0.087080   \n",
       "expectation -0.070635  0.001386 -0.159009 -0.009053 -0.033304  0.066230   \n",
       "comment     -0.048926  0.073279 -0.157980 -0.029615 -0.038201  0.044453   \n",
       "piece_of    -0.096120  0.132523 -0.236465  0.021174  0.035524  0.045692   \n",
       "invite      -0.072517  0.006181 -0.176168 -0.060853 -0.057147  0.044378   \n",
       "chair       -0.052350  0.061911 -0.186854 -0.038936 -0.036635  0.034621   \n",
       "king        -0.065018  0.099747 -0.191130 -0.021468 -0.034669  0.053461   \n",
       "parking     -0.048379  0.075182 -0.149978 -0.059116 -0.068502  0.056446   \n",
       "yeah        -0.060796  0.055035 -0.180568 -0.026316 -0.027469  0.064162   \n",
       "ice_cream   -0.083011  0.070246 -0.219129  0.008788  0.005216  0.064245   \n",
       "locate_in   -0.073498  0.051965 -0.178842 -0.039867 -0.031053  0.074643   \n",
       "fast_food   -0.052160  0.062297 -0.164139 -0.053399 -0.059141  0.086266   \n",
       "simply      -0.072506  0.057031 -0.197492 -0.011134 -0.023997  0.063496   \n",
       "double      -0.071452  0.079557 -0.211514 -0.001717 -0.014139  0.044258   \n",
       "obviously   -0.037511  0.034547 -0.131934 -0.027970 -0.069343  0.032292   \n",
       "sample      -0.071432  0.104777 -0.201297 -0.034264 -0.012804  0.029755   \n",
       "thai        -0.087092  0.065034 -0.187905  0.006958 -0.029168  0.124098   \n",
       "delish      -0.080999  0.044646 -0.216387 -0.007737 -0.022385  0.060625   \n",
       "ball        -0.098353  0.106936 -0.207373  0.010371  0.001976  0.046792   \n",
       "bagel       -0.087373  0.090548 -0.197255 -0.005157 -0.004948  0.067762   \n",
       "mexican     -0.079592  0.068700 -0.180331 -0.013603 -0.024326  0.093638   \n",
       "rock        -0.061515  0.110698 -0.216987 -0.009525  0.006905  0.059736   \n",
       "\n",
       "                   97        98        99  \n",
       "after       -0.042620  0.081347 -0.062028  \n",
       "for         -0.085593  0.092369 -0.071842  \n",
       "what        -0.042069  0.060030  0.068329  \n",
       "feel_like   -0.056412  0.143357 -0.030122  \n",
       "hubby       -0.060025  0.147040 -0.029602  \n",
       "-PRON-      -0.146688  0.106643  0.109947  \n",
       "decide_to   -0.076644  0.127589 -0.034433  \n",
       "grab        -0.028484  0.070502 -0.007932  \n",
       "drink        0.014766  0.097616  0.051688  \n",
       "at          -0.053636  0.158281 -0.181152  \n",
       "the          0.024243  0.236098  0.060681  \n",
       "bar          0.067654  0.082698 -0.024255  \n",
       "be          -0.046506  0.221583 -0.083481  \n",
       "that        -0.093088  0.107353 -0.076216  \n",
       "restaurant   0.004933 -0.013864  0.031861  \n",
       "worth       -0.079585  0.112231 -0.042929  \n",
       "check_out   -0.090479  0.131652 -0.038950  \n",
       "but         -0.084684  0.144807 -0.025015  \n",
       "just        -0.098639  0.125218 -0.079987  \n",
       "not         -0.148049 -0.013735 -0.019713  \n",
       "feel        -0.083228  0.154591 -0.014529  \n",
       "up          -0.099835  0.161015 -0.049040  \n",
       "to          -0.103029  0.123279  0.022389  \n",
       "do          -0.135276  0.194920 -0.081466  \n",
       "bartender    0.025731  0.134976  0.052527  \n",
       "extremely    0.021176  0.166077  0.051639  \n",
       "friendly     0.053178  0.109140  0.082653  \n",
       "offer       -0.045496  0.185183 -0.069647  \n",
       "about       -0.084110  0.169992 -0.141422  \n",
       "try         -0.201801  0.104740 -0.087347  \n",
       "...               ...       ...       ...  \n",
       "orange      -0.017234  0.196826 -0.021115  \n",
       "hot_dog     -0.051539  0.188855 -0.025081  \n",
       "bun         -0.049272  0.174500 -0.031476  \n",
       "explain     -0.056738  0.154011 -0.046166  \n",
       "raw         -0.010847  0.181290 -0.043763  \n",
       "date        -0.046828  0.117739 -0.023151  \n",
       "long_time   -0.048727  0.119697  0.000865  \n",
       "pho         -0.059018  0.129633 -0.033448  \n",
       "broth       -0.038526  0.175840 -0.013471  \n",
       "expectation -0.083825  0.149617 -0.040977  \n",
       "comment     -0.051404  0.146514 -0.059833  \n",
       "piece_of     0.005880  0.197716 -0.031886  \n",
       "invite      -0.014952  0.153506  0.000085  \n",
       "chair       -0.047665  0.152779 -0.044512  \n",
       "king        -0.040530  0.157559 -0.039047  \n",
       "parking     -0.055794  0.063947 -0.030707  \n",
       "yeah        -0.067593  0.164537 -0.039014  \n",
       "ice_cream   -0.022119  0.198504 -0.030411  \n",
       "locate_in   -0.056070  0.131874 -0.064682  \n",
       "fast_food   -0.042504  0.126710 -0.027194  \n",
       "simply      -0.040919  0.175969 -0.015342  \n",
       "double      -0.057092  0.163729 -0.025052  \n",
       "obviously   -0.081633  0.102622 -0.015472  \n",
       "sample      -0.049235  0.181356 -0.050868  \n",
       "thai        -0.031383  0.143490  0.004485  \n",
       "delish      -0.046491  0.168931 -0.011795  \n",
       "ball        -0.035770  0.157458 -0.042737  \n",
       "bagel       -0.046329  0.171968 -0.024236  \n",
       "mexican     -0.061941  0.162244 -0.035284  \n",
       "rock        -0.031415  0.202909 -0.048059  \n",
       "\n",
       "[1024 rows x 100 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in food2vec.wv.vocab.items()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "#ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the food2vec vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(food2vec.wv.syn0norm[term_indices, :],\n",
    "                            index=ordered_terms)\n",
    "\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy wall of numbers! This DataFrame has 50,835 rows &mdash; one for each term in the vocabulary &mdash; and 100 colums. Our model has learned a quantitative vector representation for each term, as expected.\n",
    "\n",
    "Put another way, our model has \"embedded\" the terms into a 100-dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... what can we do with all these numbers?\n",
    "The first thing we can use them for is to simply look up related words and phrases for a given term of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=10):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in food2vec.most_similar(positive=[token], topn=topn):\n",
    "\n",
    "        print (u'{:20} {}'.format(word, round(similarity, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What things are like Burger King?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount_of            0.962\n",
      "cocktail             0.958\n",
      "wine                 0.958\n",
      "as_well_as           0.954\n",
      "classic              0.954\n",
      "variety_of           0.952\n",
      "selection_of         0.948\n",
      "come_out             0.942\n",
      "presentation         0.94\n",
      "incredible           0.94\n"
     ]
    }
   ],
   "source": [
    "get_related_terms(u'beer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned that fast food restaurants are similar to each other! In particular, *mcdonalds* and *wendy's* are the most similar to Burger King, according to this dataset. In addition, the model has found that alternate spellings for the same entities are probably related, such as *mcdonalds*, *mcdonald's* and *mcd's*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When is happy hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total                0.971\n",
      "tuesday              0.97\n",
      "date                 0.964\n",
      "friday               0.963\n",
      "pm                   0.962\n",
      "booth                0.962\n",
      "group_of             0.962\n",
      "summer               0.962\n",
      "11                   0.96\n",
      "game                 0.958\n"
     ]
    }
   ],
   "source": [
    "get_related_terms(u'happy_hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has noticed several alternate spellings for happy hour, such as *hh* and *happy hr*, and assesses them as highly related. If you were looking for reviews about happy hour, such alternate spellings would be very helpful to know.\n",
    "\n",
    "Taking a deeper look &mdash; the model has turned up phrases like *3-6pm*, *4-7pm*, and *mon-fri*, too. This is especially interesting, because the model has no advance knowledge at all about what happy hour is, and what time of day it should be. But simply by scanning through restaurant reviews, the model has discovered that the concept of happy hour has something very important to do with that block of time around 3-7pm on weekdays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make pasta tonight. Which style do you want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baked                0.985\n",
      "sour                 0.985\n",
      "fry_rice             0.985\n",
      "duck                 0.984\n",
      "cream                0.984\n",
      "thick                0.984\n",
      "bit                  0.984\n",
      "turkey               0.984\n",
      "tofu                 0.984\n",
      "batter               0.984\n",
      "soft                 0.983\n",
      "meatball             0.982\n",
      "white                0.982\n",
      "mango                0.982\n",
      "crab                 0.981\n",
      "gravy                0.98\n",
      "chocolate            0.98\n",
      "lamb                 0.98\n",
      "thin                 0.98\n",
      "seafood              0.98\n"
     ]
    }
   ],
   "source": [
    "get_related_terms(u'pasta', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word algebra!\n",
    "No self-respecting word2vec demo would be complete without a healthy dose of *word algebra*, also known as *analogy completion*.\n",
    "\n",
    "The core idea is that once words are represented as numerical vectors, you can do math with them. The mathematical procedure goes like this:\n",
    "1. Provide a set of words or phrases that you'd like to add or subtract.\n",
    "1. Look up the vectors that represent those terms in the word vector model.\n",
    "1. Add and subtract those vectors to produce a new, combined vector.\n",
    "1. Look up the most similar vector(s) to this new, combined vector via cosine similarity.\n",
    "1. Return the word(s) associated with the similar vector(s).\n",
    "\n",
    "But more generally, you can think of the vectors that represent each word as encoding some information about the *meaning* or *concepts* of the word. What happens when you ask the model to combine the meaning and concepts of words in new ways? Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=1):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add= and subtract=, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    \"\"\"\n",
    "    answers = food2vec.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print (term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### breakfast + lunch = ?\n",
    "Let's start with a softball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinner\n"
     ]
    }
   ],
   "source": [
    "word_algebra(add=[u'breakfast', u'lunch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so the model knows that *brunch* is a combination of *breakfast* and *lunch*. What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lunch - day + night = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinner\n"
     ]
    }
   ],
   "source": [
    "word_algebra(add=[u'lunch', u'night'], subtract=[u'day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting a bit more nuanced. The model has discovered that:\n",
    "- Both *lunch* and *dinner* are meals\n",
    "- The main difference between them is time of day\n",
    "- Day and night are times of day\n",
    "- Lunch is associated with day, and dinner is associated with night\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taco - mexican + chinese = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomato\n"
     ]
    }
   ],
   "source": [
    "word_algebra(add=[u'taco', u'chinese'], subtract=[u'mexican'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an entirely new and different type of relationship that the model has learned.\n",
    "- It knows that tacos are a characteristic example of Mexican food\n",
    "- It knows that Mexican and Chinese are both styles of food\n",
    "- If you subtract *Mexican* from *taco*, you're left with something like the concept of a *\"characteristic type of food\"*, which is represented as a new vector\n",
    "- If you add that new *\"characteristic type of food\"* vector to Chinese, you get *dumpling*.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bun - american + mexican = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fry_chicken\n"
     ]
    }
   ],
   "source": [
    "word_algebra(add=[u'bun', u'mexican'], subtract=[u'american'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model knows that both *buns* and *tortillas* are the doughy thing that goes on the outside of your real food, and that the primary difference between them is the style of food they're associated with.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could do this all day. One last analogy before we move on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! Let's round up the major components that we've seen:\n",
    "1. Text processing with **spaCy**\n",
    "1. Automated **phrase modeling**\n",
    "1. Word vector modeling with **word2vec**\n",
    "\n",
    "#### Why use these models?\n",
    "Dense vector representations for text like LDA and word2vec can greatly improve performance for a number of common, text-heavy problems like:\n",
    "- Text classification\n",
    "- Search\n",
    "- Recommendations\n",
    "- Question answering\n",
    "\n",
    "...and more generally are a powerful way machines can help humans make sense of what's in a giant pile of text. They're also often useful as a pre-processing step for many other downstream machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
